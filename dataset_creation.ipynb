{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "\n",
    "![flowchart](./resources/dataset_creation_flowchart.drawio.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.file_utils import open_json, write_json\n",
    "from utils.dataset_creation import *\n",
    "from utils.dataset_mapping import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "UPDATE_MAPPING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"pdbs\", \"uniprot\", \"wild_aa\", \"mutation_position\",\n",
    "           \"mutated_aa\", \"pH\",\n",
    "           \"sequence\", \"length\", \"chain_start\", \"chain_end\",\n",
    "           \"AlphaFoldDB\", \"Tm\", \"ddG\", \"dTm\",\n",
    "           \"dataset_source\", \"infos_found\"]\n",
    "\n",
    "SUBSET_DUPLICATES = [\"uniprot\", \"wild_aa\", \"mutation_position\",\n",
    "                     \"mutated_aa\", \"pH\", \"sequence\"]\n",
    "\n",
    "NAME = \"all\"\n",
    "DIR = \"./data/main_dataset_creation\"\n",
    "OUTPUT_DIR = DIR+'/outputs/'+NAME\n",
    "\n",
    "LOCAL_UNIPROT_INFOS_PATH = DIR+\"/uniprot_infos.json\"\n",
    "PDB_UNIPROT_MAPPING_PATH = DIR+\"/mapping/pdb_uniprot_mapping.json\"\n",
    "LINKED_UNIPROT_MAPPING_PATH = DIR+\"/mapping/linked_uniprot_mapping.json\"\n",
    "SEQUENCE_UNIPROT_MAPPING_PATH = DIR + \\\n",
    "    \"/mapping/sequence_uniprot_mapping.json\"\n",
    "PDB_NO_UNIPROT_PATH = DIR+\"/mapping/pdb_no_uniprot.json\"\n",
    "SEQUENCE_NO_UNIPROT_PATH = DIR+\"/mapping/sequence_no_uniprot.json\"\n",
    "\n",
    "DATASET_OUTPUT_PATH_RAW = OUTPUT_DIR+f\"/dataset_raw.csv\"\n",
    "DATASET_OUTPUT_PATH_ONLY_INFOS = OUTPUT_DIR+f\"/dataset_only_infos.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infos for dataset creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 578 uniprot infos from local storage\n"
     ]
    }
   ],
   "source": [
    "local_uniprot_infos = open_json(LOCAL_UNIPROT_INFOS_PATH)\n",
    "dataset_config = open_json(DIR+\"/dataset_config.json\")\n",
    "\n",
    "print(f\"loaded {len(local_uniprot_infos)} uniprot infos from local storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare output dir\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    print(f\"creating {OUTPUT_DIR} folder\")\n",
    "    os.mkdir(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all the required dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed fireprotdb_curated:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 222, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed fireprotdb_not_curated:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 5, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "11201\n",
      "processed thermomutdb:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 2651, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed O2567_new:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed prothermdb:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 1451, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed jinyuan_sun_train:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed jinyuan_sun_test:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed datasetDDG_train:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 1527, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n",
      "processed datasetDDG_test:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 73, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not FRESH_START:\n",
    "    main_df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "else:\n",
    "    main_df = pd.DataFrame()\n",
    "    main_df = add_missing_column(main_df, COLUMNS)\n",
    "\n",
    "    for dataset_source in dataset_config[\"dataset_to_process\"]:\n",
    "        errors = {\n",
    "            \"no_sequence_in_data\": 0,\n",
    "            \"not_in_local\": 0,\n",
    "            \"wrong_position\": 0,\n",
    "            \"no_uniprot\": 0,\n",
    "            \"no_pdb\": 0,\n",
    "            \"no_sequence\": 0,\n",
    "        }\n",
    "\n",
    "        individual_config = dataset_config[dataset_source]\n",
    "        # load csv\n",
    "        if dataset_source == \"thermomutdb\":\n",
    "            df = pd.read_json(individual_config[\"data_path\"])\n",
    "            df = df[df.mut_count.eq(0)]\n",
    "            df[df.uniprot.eq('-')] = np.nan\n",
    "            print(len(df))\n",
    "        else:\n",
    "            df = pd.read_csv(individual_config[\"data_path\"])\n",
    "        # rename columns\n",
    "        df.rename(columns=individual_config[\"renaming_dict\"],\n",
    "                inplace=True)\n",
    "        # add missing columns\n",
    "        df = add_missing_column(df, COLUMNS)\n",
    "        # split mutation code if needed\n",
    "        if individual_config[\"need_mutation_code_split\"]:\n",
    "            df = df.apply(apply_split_mutation_code, axis=1)\n",
    "        # remove nan mutation_code\n",
    "        df = df[~df[\"mutation_position\"].isna()]\n",
    "        # keep only COLUMNS\n",
    "        df = df[COLUMNS]\n",
    "        # drop duplicates\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        # add dataset_source\n",
    "        df[\"dataset_source\"] = dataset_source\n",
    "        # index start at 0\n",
    "        df[\"mutation_position\"] = df[\"mutation_position\"].apply(lambda x: x-1)\n",
    "        # apply target corrections\n",
    "        df[\"ddG\"] = df[\"ddG\"].apply(\n",
    "            lambda x: x*individual_config[\"corrections\"][\"ddG\"])\n",
    "        df[\"dTm\"] = df[\"dTm\"].apply(\n",
    "            lambda x: x*individual_config[\"corrections\"][\"dTm\"])\n",
    "        # better to initialize infos_found at 0 than nan\n",
    "        df[\"infos_found\"] = 0\n",
    "        \n",
    "        # check number of rows without uniprot\n",
    "        # check validity of uniprot, and add the infos for those\n",
    "        df = df.apply(lambda row: apply_valid_uniprot(\n",
    "            row, local_uniprot_infos, dataset_config, errors), axis=1)\n",
    "        \n",
    "        \n",
    "        print(f\"processed {dataset_source}:\")\n",
    "        print(f\"{errors=}\\n\")\n",
    "\n",
    "        main_df = pd.concat([main_df, df], ignore_index=True)\n",
    "        main_df.drop_duplicates(SUBSET_DUPLICATES, inplace=True)\n",
    "\n",
    "    # save\n",
    "    write_json(LOCAL_UNIPROT_INFOS_PATH, local_uniprot_infos)\n",
    "    main_df.to_csv(DATASET_OUTPUT_PATH_RAW, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update mapping and try to add infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not UPDATE_MAPPING:\n",
    "    # don't go beyond here with Run All\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge pdb_uniprot_mapping\n",
    "# pdb_uniprot_mapping = open_json(PDB_UNIPROT_MAPPING_PATH)\n",
    "# pdb_uniprot_mapping2 = open_json(\n",
    "#     \"./data/main_dataset_creation/mapping/pdb_uniprot_mapping_2.json\")\n",
    "\n",
    "# l = len(pdb_uniprot_mapping)\n",
    "# for pdb, mapped_uniprot in pdb_uniprot_mapping2.items():\n",
    "#     if pdb not in pdb_uniprot_mapping:\n",
    "#         pdb_uniprot_mapping[pdb] = mapped_uniprot\n",
    "\n",
    "# print(f\"added {len(pdb_uniprot_mapping)-l} new pdb mapping via merge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 0 entries to pdb_uniprot_mapping\n"
     ]
    }
   ],
   "source": [
    "# update pdb to uniprot mapping\n",
    "update_pdb_uniprot_mapping(LOCAL_UNIPROT_INFOS_PATH,\n",
    "                           PDB_UNIPROT_MAPPING_PATH,\n",
    "                           LINKED_UNIPROT_MAPPING_PATH)\n",
    "\n",
    "pdb_uniprot_mapping = open_json(PDB_UNIPROT_MAPPING_PATH)\n",
    "linked_uniprot_mapping = open_json(LINKED_UNIPROT_MAPPING_PATH)\n",
    "pdb_without_uniprot = open_json(PDB_NO_UNIPROT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 2433.0 new infos thanks to uniprot_from_pdb\n"
     ]
    }
   ],
   "source": [
    "# add infos based on pdb not uniprot\n",
    "df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "\n",
    "with_infos = df.infos_found.sum()\n",
    "df = df.apply(lambda row: apply_infos_from_pdb(row, local_uniprot_infos, pdb_uniprot_mapping,\n",
    "                                               linked_uniprot_mapping, dataset_config,\n",
    "                                               pdb_without_uniprot, errors),\n",
    "              axis=1)\n",
    "print(\n",
    "    f\"added {df.infos_found.sum()-with_infos} new infos thanks to uniprot_from_pdb\")\n",
    "df.to_csv(DATASET_OUTPUT_PATH_RAW, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 0 entries to sequence_uniprot_mapping\n"
     ]
    }
   ],
   "source": [
    "# update sequence to uniprot mapping\n",
    "update_sequence_uniprot_mapping(LOCAL_UNIPROT_INFOS_PATH,\n",
    "                                SEQUENCE_UNIPROT_MAPPING_PATH,\n",
    "                                LINKED_UNIPROT_MAPPING_PATH)\n",
    "\n",
    "sequence_uniprot_mapping = open_json(SEQUENCE_UNIPROT_MAPPING_PATH)\n",
    "sequence_without_uniprot = open_json(SEQUENCE_NO_UNIPROT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 0.0 new infos thanks to uniprot_from_sequence\n"
     ]
    }
   ],
   "source": [
    "# add infos based on sequence not pdb or uniprot\n",
    "\n",
    "df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "\n",
    "with_infos = df.infos_found.sum()\n",
    "df = df.apply(lambda row: apply_infos_from_sequence(row, local_uniprot_infos, sequence_uniprot_mapping,\n",
    "                                                    linked_uniprot_mapping, dataset_config,\n",
    "                                                    sequence_without_uniprot, errors),\n",
    "              axis=1)\n",
    "print(\n",
    "    f\"added {df.infos_found.sum()-with_infos} new infos thanks to uniprot_from_sequence\")\n",
    "\n",
    "df.to_csv(DATASET_OUTPUT_PATH_RAW, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure mapping and other data is saved\n",
    "write_json(LOCAL_UNIPROT_INFOS_PATH, local_uniprot_infos)\n",
    "write_json(PDB_NO_UNIPROT_PATH, pdb_without_uniprot)\n",
    "write_json(SEQUENCE_NO_UNIPROT_PATH, sequence_without_uniprot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7589\n",
      "23277\n",
      "15688\n",
      "6.504306897326444\n"
     ]
    }
   ],
   "source": [
    "main_df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "dataset_config = open_json(DIR+\"/dataset_config.json\")\n",
    "# remove pdbs\n",
    "main_df = main_df[COLUMNS[1:]]\n",
    "print(main_df.infos_found.eq(0.0).sum())\n",
    "\n",
    "# remove record without uniprot infos\n",
    "print(len(main_df))\n",
    "main_df = main_df.loc[main_df.infos_found == 1]\n",
    "print(len(main_df))\n",
    "# fill na pH if in config, otherwise remove those\n",
    "if dataset_config[\"general_config\"][\"fill_na_pH\"]:\n",
    "    mean_pH = main_df.pH.mean()\n",
    "    print(mean_pH)\n",
    "    main_df.pH.fillna(mean_pH, inplace=True)\n",
    "else:\n",
    "    print(len(main_df))\n",
    "    main_df = main_df.loc[~[main_df.pH.isna()]]\n",
    "    print(len(main_df))\n",
    "\n",
    "dataset_infos = {\n",
    "    \"total_len\": len(main_df),\n",
    "    \"dataset_processed\": dataset_config[\"dataset_to_process\"],\n",
    "    \"general_config\": dataset_config[\"general_config\"],\n",
    "    \"dataset_source_repartition\": main_df.dataset_source.value_counts().to_dict(),\n",
    "    \"unique_uniprot\": len(main_df.uniprot.unique()),\n",
    "    \"ddG\": (len(main_df)-main_df.ddG.isna().sum()),\n",
    "    \"dTm\": (len(main_df)-main_df.dTm.isna().sum()),\n",
    "    \"Tm\": (len(main_df)-main_df.Tm.isna().sum()),\n",
    "    \"nan_repartition\": main_df.isna().sum().to_dict(),\n",
    "    \"no_pH_repartition\": main_df[main_df.pH.isna()].dataset_source.value_counts().to_dict(),\n",
    "}\n",
    "\n",
    "main_df.to_csv(DATASET_OUTPUT_PATH_ONLY_INFOS, index=False)\n",
    "write_json(OUTPUT_DIR+\"/dataset_config.json\", dataset_config)\n",
    "write_json(OUTPUT_DIR+\"/dataset_infos.json\", dataset_infos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_len': 15688, 'dataset_processed': ['fireprotdb_curated', 'fireprotdb_not_curated', 'thermomutdb', 'O2567_new', 'prothermdb', 'jinyuan_sun_train', 'jinyuan_sun_test', 'datasetDDG_train', 'datasetDDG_test'], 'general_config': {'fill_na_pH': True}, 'dataset_source_repartition': {'fireprotdb_curated': 6329, 'thermomutdb': 3316, 'fireprotdb_not_curated': 2614, 'O2567_new': 1359, 'prothermdb': 1087, 'datasetDDG_train': 519, 'jinyuan_sun_train': 405, 'datasetDDG_test': 53, 'jinyuan_sun_test': 6}, 'unique_uniprot': 505, 'ddG': 10540, 'dTm': 4888, 'Tm': 4467, 'nan_repartition': {'uniprot': 0, 'wild_aa': 0, 'mutation_position': 0, 'mutated_aa': 0, 'pH': 0, 'sequence': 0, 'length': 0, 'chain_start': 0, 'chain_end': 0, 'AlphaFoldDB': 2051, 'Tm': 11221, 'ddG': 5148, 'dTm': 10800, 'dataset_source': 0, 'infos_found': 0}, 'no_pH_repartition': {}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_infos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('novozymes-prediction-Gl9CRTFV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b13dc4add9efa918e6bb920c50afa2240557655d90455391ab57f21c65447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
