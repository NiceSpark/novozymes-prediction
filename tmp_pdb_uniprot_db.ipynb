{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a cross id db for pdb uniprot and basic infos abt the protein\n",
    "see [./data/sources.csv](./data/sources.csv) for the different sources where we were able to download each datasets\n",
    "\n",
    "This notebook reuse the code in [individual_datasets_creation](./individual_datasets_creation.ipynb) NB.\n",
    "The goal is to have a helping DB for merging, duplicate search and already known sequence search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pypdb\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"uniprot\", \"PDB_wild\", \"sequence\", \"length\", \"molWeight\", \"countByFeatureType\", \"chain_start\", \"chain_end\"]\n",
    "# \"mutation_sequence_code\"\n",
    "# turn to False if you want to save all available info in the db\n",
    "SAVE_ONLY_COLUMNS = True\n",
    "\n",
    "main_df = pd.DataFrame(columns=COLUMNS)\n",
    "\n",
    "def add_missing_column(df):\n",
    "    for name in COLUMNS:\n",
    "        if name not in df.columns.to_list():\n",
    "            df[name] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_df(df, name):\n",
    "    if SAVE_ONLY_COLUMNS:\n",
    "        df = df[COLUMNS]\n",
    "    df.to_csv(f\"./data/main_dataset/{name}.csv\", index=False)\n",
    "\n",
    "def add_infos(main_df, df):\n",
    "    if SAVE_ONLY_COLUMNS:\n",
    "        df = df[COLUMNS]\n",
    "    main_df = pd.concat([main_df, df], ignore_index=True)\n",
    "    main_df = main_df.drop_duplicates()\n",
    "    main_df.reset_index(inplace=True)\n",
    "    main_df.drop(columns=\"index\", inplace=True)\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "##### Ssym+ #####\n",
    "\n",
    "# load Ssym+ and remove predictions and index columns\n",
    "columns_experimental = [\"Protein\",\"Mut_pdb\",\"DDG_dir\",\"DDG_inv\",\"DDG\",\"Ph\",\"T\",\"Mut_Seq\",\"Protein_inv\",\"Mut_pdb_inv\",\"Mut_Seq_inv\"]\n",
    "ssym_dir_df = pd.read_csv(\"./data/Ssym+/Ssym+_experimental.csv\")[columns_experimental]\n",
    "\n",
    "\n",
    "ssym_dir_df = add_missing_column(ssym_dir_df)\n",
    "\n",
    "def process_prot_name(row):\n",
    "    name = row[\"Protein\"]\n",
    "    row[\"PDB_wild\"]=name[:-1].upper()\n",
    "    row[\"mutated_chain\"]=name[-1]\n",
    "    return row\n",
    "\n",
    "ssym_dir_df = ssym_dir_df.apply(process_prot_name, axis=1)\n",
    "\n",
    "main_df = add_infos(main_df, ssym_dir_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "##### ThermoMutDB #####\n",
    "\n",
    "thermomut_df = pd.read_json(\"./data/ThermoMutDB/thermomutdb.json\")\n",
    "\n",
    "thermomut_df = add_missing_column(thermomut_df)\n",
    "main_df = add_infos(main_df, thermomut_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n"
     ]
    }
   ],
   "source": [
    "##### O2567 #####\n",
    "\n",
    "o2567_df = pd.read_csv(\"./data/O2567_new/O2567_new.csv\")\n",
    "o2567_df = o2567_df.rename(columns={\"PDB code\": \"PDB_wild\"})\n",
    "\n",
    "o2567_df = add_missing_column(o2567_df)\n",
    "main_df = add_infos(main_df, o2567_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n"
     ]
    }
   ],
   "source": [
    "##### Jynyuan Sun's dataset (Phd. student in bio) #####\n",
    "\n",
    "\"pdb,wildtype,pdb_resseq,seq_index,mutation,wt_seq,mut_seq,ddG\"\n",
    "v2_test_df = pd.read_csv(\"./data/jinyuan_sun/v2/test.csv\")\n",
    "\"pdb,wildtype,pdb_resseq,seq_index,mutation,wt_seq,mut_seq,ddG,group\"\n",
    "v2_train_df = pd.read_csv(\"./data/jinyuan_sun/v2/train.csv\")\n",
    "\n",
    "v2_train_df.drop(columns=[\"group\"], axis=1, inplace=True)\n",
    "\n",
    "jinyuan_sun_df = pd.concat([v2_test_df, v2_train_df], ignore_index=True)\n",
    "jinyuan_sun_df.rename(columns={\"pdb\": \"PDB_wild\", \"wt_seq\": \"sequence\"})\n",
    "\n",
    "\n",
    "jinyuan_sun_df = add_missing_column(jinyuan_sun_df)\n",
    "main_df = add_infos(main_df, jinyuan_sun_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "##### FireProtDB #####\n",
    "\n",
    "# csv obtained by searching all values with \"has_ddg\"\n",
    "fire_df = pd.read_csv(\"./data/FireProtDB/fireprotdb_has_ddg_or_dtm_is_curated.csv\")\n",
    "\n",
    "# Keep only curated data\n",
    "fire_df = fire_df[fire_df[\"is_curated\"]]\n",
    "\n",
    "# Drop duplicate rows\n",
    "# THIS REMOVE 14k+ entries !!!!\n",
    "duplicate_subset = [\"pdb_id\", \"dTm\", \"ddG\", \"chain\", \"wild_type\", \"position\", \"mutation\", \"sequence\"]\n",
    "fire_df = fire_df.drop_duplicates(duplicate_subset)\n",
    "\n",
    "# Drop rows with no valid pdb files ~10-15\n",
    "fire_df = fire_df[~pd.isna(fire_df.pdb_id)]\n",
    "# Remove weird duplicate pdb ids in the pdb_id column (keep '|' for now)\n",
    "fire_df[\"pdb_id\"] = fire_df[\"pdb_id\"].apply(lambda x: x if \"|\" not in x else \"|\".join(list(set(x.split(\"|\")))))\n",
    "\n",
    "# fire_df.columns.to_list()\n",
    "fire_df.rename(columns={\"pdb_id\": \"PDB_wild\", \"uniprot_id\": \"uniprot\"}, inplace=True)\n",
    "\n",
    "fire_df = add_missing_column(fire_df)\n",
    "main_df = add_infos(main_df, fire_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "##### DeepDDG dataset #####\n",
    "train_df = pd.read_csv(\"./data/DeepDDG_train_dataset/datasetDDG_train.csv\")\n",
    "test_df = pd.read_csv(\"./data/DeepDDG_train_dataset/datasetDDG_test.csv\")\n",
    "\n",
    "train_df.drop(columns=\"Fold ID in 10-fold cross validation\", axis=1, inplace= True)\n",
    "deepddg_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "deepddg_df.rename(columns={\"PDB ID with modifications to be made\": \"PDB_wild\", \"Uniprot ID\": \"uniprot\"}, \n",
    "                            inplace= True)\n",
    "\n",
    "deepddg_df = add_missing_column(deepddg_df)\n",
    "main_df = add_infos(main_df, fire_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "##### ProThermDB #####\n",
    "all_ddg_df = pd.read_csv(\"./data/ProThermDB/prothermdb_all_ddg.tsv\", sep=\"\\t\")\n",
    "all_dtm_df = pd.read_csv(\"./data/ProThermDB/prothermdb_all_dTm.tsv\", sep=\"\\t\")\n",
    "all_tm_df = pd.read_csv(\"./data/ProThermDB/prothermdb_all_Tm.tsv\", sep=\"\\t\")\n",
    "\n",
    "prothermdb_df = pd.concat([all_ddg_df, all_dtm_df, all_tm_df], ignore_index=True)\n",
    "\n",
    "# removes duplicate: ~400 rows\n",
    "prothermdb_df = prothermdb_df.drop_duplicates()\n",
    "\n",
    "prothermdb_df.rename(columns={\"UniProt_ID\": \"uniprot\"}, \n",
    "                    inplace= True)\n",
    "\n",
    "prothermdb_df = add_missing_column(prothermdb_df)\n",
    "\n",
    "def process_prothermdb(row):\n",
    "    for k in row.keys():\n",
    "        if row[k]=='-':\n",
    "            row[k]=\"\"\n",
    "    \n",
    "    # coherent PDB_WILD: all caps\n",
    "    row[\"PDB_wild\"] = row[\"PDB_wild\"].upper()\n",
    "    row[\"uniprot\"] = row[\"uniprot\"].replace(\" \", \"\")\n",
    "    return row\n",
    "\n",
    "prothermdb_df = prothermdb_df.apply(process_prothermdb, axis=1)\n",
    "main_df = add_infos(main_df, fire_df)\n",
    "print(len(main_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(main_df, \"tmp_pdb_uniprot_db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('novozymes-prediction-Gl9CRTFV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b13dc4add9efa918e6bb920c50afa2240557655d90455391ab57f21c65447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
