{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take pdb_uniprot_db.csv and add the uniprot ids and infos when missing\n",
    "\n",
    "1st step: merge row together (only 1 row for each pdb/uniprot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pypdb\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS=['uniprot', 'PDB_wild', 'sequence', 'length', 'molWeight', 'countByFeatureType', 'chain_start', 'chain_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_pdb(seq):\n",
    "    # get pdb id from protein sequence using the pypdb package to query the RCSB Protein Data Bank API\n",
    "    q = pypdb.Query(seq, \n",
    "        query_type=\"sequence\", \n",
    "        return_type=\"polymer_entity\")\n",
    "    \n",
    "    for result in q.search()[\"result_set\"]:\n",
    "        [result_id, chain] = result[\"identifier\"].split('_')\n",
    "        if result[\"score\"] == 1.0 and chain==\"1\":\n",
    "            return result_id\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def uniprotid_to_infos(uniprotid, row):\n",
    "    with urllib.request.urlopen(\"https://rest.uniprot.org/uniprotkb/P00282.json\") as url:\n",
    "        data = json.load(url)\n",
    "\n",
    "    features = data.get(\"features\", [])\n",
    "    chain_location = next((x for x in features if x[\"type\"]==\"Chain\"), {}).get(\"location\", {})\n",
    "    return {\n",
    "        \"sequence\": data.get(\"sequence\", {}).get(\"value\"),\n",
    "        \"length\": data.get(\"sequence\", {}).get(\"length\"),\n",
    "        \"molWeight\": data.get(\"sequence\", {}).get(\"molWeight\"),\n",
    "        \"countByFeatureType\": data.get(\"extraAttributes\", {}).get(\"countByFeatureType\"), \n",
    "        \"chain_start\": chain_location.get(\"start\")[\"value\"],\n",
    "        \"chain_end\": chain_location.get(\"end\")[\"value\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot</th>\n",
       "      <th>PDB_wild</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1AMQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1ARR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot PDB_wild\n",
       "0     NaN     1AMQ\n",
       "1     NaN     1ARR"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/main_dataset/pdb_uniprot_db.csv\")\n",
    "df = df[[\"uniprot\", \"PDB_wild\"]]\n",
    "df[\"PDB_wild\"] = df[\"PDB_wild\"].apply(lambda x: str(x).upper())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649\n",
      "730\n",
      "794\n",
      "rm 13 rows from no_uniprot because at least 1 other row contained both information\n",
      "rm 8 rows from no_pdb because at least 1 other row contained both information\n",
      "772\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "def agg_function(l):\n",
    "    unique_l = set([str(x) for x in l if (x and str(x)!='nan')])\n",
    "    return \" \".join(unique_l)\n",
    "\n",
    "df2 = df.groupby(\"PDB_wild\", as_index=False).agg({\"uniprot\": agg_function})\n",
    "print(len(df2))\n",
    "\n",
    "\n",
    "#### MULTIPLE uniprot ids ####\n",
    "# (X,P00644 P00645) => (X, P00644), (X, P00645)\n",
    "# add rows when there are multiple uniprot id for 1 pdb id\n",
    "multiple_uniprot = df2['uniprot'].str.contains(\" \", case=False)\n",
    "multiple_uniprot_df = pd.DataFrame()\n",
    "for _, row in df2.loc[multiple_uniprot].iterrows():\n",
    "    uniprot_ids = row.uniprot.split(\" \")\n",
    "    additional_rows = pd.DataFrame({\"PDB_wild\": [row.PDB_wild if row.PDB_wild != \"NAN\" else \"\"]*len(uniprot_ids),\n",
    "                                    \"uniprot\": uniprot_ids\n",
    "                                    }, columns=df2.columns)\n",
    "    multiple_uniprot_df = pd.concat([multiple_uniprot_df, additional_rows], ignore_index=True)\n",
    "\n",
    "df2 = df2.loc[~multiple_uniprot]\n",
    "df2 = pd.concat([df2, multiple_uniprot_df], ignore_index=True)\n",
    "print(len(df2))\n",
    "\n",
    "#### MULTIPLE pdb ids ####\n",
    "# (1SVX|3MBP,P0AEX9) => (1SVX,P0AEX9), (3MBP, P0AEX9)\n",
    "multiple_pdb = df2[\"PDB_wild\"].str.contains(\"|\", case=False)\n",
    "multiple_pdb_df = pd.DataFrame()\n",
    "for _, row in df2.loc[multiple_pdb].iterrows():\n",
    "    multiple_pdb_ids = row.PDB_wild.split(\"|\")\n",
    "    additional_rows = pd.DataFrame({\"PDB_wild\": multiple_pdb_ids,\n",
    "                                    \"uniprot\": [row.uniprot]*len(multiple_pdb_ids)\n",
    "                                    }, columns=df2.columns)\n",
    "    multiple_pdb_df = pd.concat([multiple_pdb_df, additional_rows], ignore_index=True)\n",
    "\n",
    "df2 = df2.loc[~multiple_pdb]\n",
    "df2 = pd.concat([df2, multiple_pdb_df], ignore_index=True)\n",
    "print(len(df2))\n",
    "\n",
    "#### Warning ####\n",
    "# A weird bug makes it so 1E21 is converted to a number (1.00E+21) and therefor is different from the row w/ PDBwild = \"1E21\"\n",
    "# Thus we remove this first line (it exist already on line 121)\n",
    "df2 = df2.iloc[1:, :]\n",
    "\n",
    "#### Remove occurences when there is a PDB with no uniprot \n",
    "# alltough another row contains the PDB & the uniprot \n",
    "# (and vice-versa) ####\n",
    "no_uniprot = df2.uniprot.eq(\"\")\n",
    "no_uniprot_df = df2.loc[no_uniprot]\n",
    "no_uniprot_df.reset_index(inplace=True)\n",
    "df2 = df2.loc[~no_uniprot]\n",
    "\n",
    "no_pdb = df2.PDB_wild.eq(\"\")\n",
    "no_pdb_df = df2.loc[no_pdb]\n",
    "no_pdb_df.reset_index(inplace=True)\n",
    "df2 = df2.loc[~no_pdb]\n",
    "\n",
    "# df2 now consist of only rows with both pdb AND uniprot\n",
    "# uniprot:\n",
    "linked_row_found = [True]*len(no_uniprot_df)\n",
    "for index, row in no_uniprot_df.iterrows():\n",
    "    linked_row = df2.PDB_wild.eq(row.PDB_wild)\n",
    "    if linked_row.any():\n",
    "        linked_row_found[index] = False\n",
    "# we remove the rows with no_uniprot for which we found another row with both uniprot and pdb\n",
    "no_uniprot_df = no_uniprot_df[linked_row_found]\n",
    "print(f\"rm {len(linked_row_found)-len(no_uniprot_df)} rows from no_uniprot because at least 1 other row contained both information\")\n",
    "# pdb:\n",
    "linked_row_found = [True]*len(no_pdb_df)\n",
    "for index, row in no_pdb_df.iterrows():\n",
    "    linked_row = df2.uniprot.eq(row.uniprot)\n",
    "    if linked_row.any():\n",
    "        linked_row_found[index] = False\n",
    "# we remove the rows with no_uniprot for which we found another row with both uniprot and pdb\n",
    "no_pdb_df = no_pdb_df[linked_row_found]\n",
    "print(f\"rm {len(linked_row_found)-len(no_pdb_df)} rows from no_pdb because at least 1 other row contained both information\")\n",
    "\n",
    "# we add back the curated rows with no_pdb and no_uniprot\n",
    "df2 = pd.concat([df2, no_pdb_df, no_uniprot_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2.to_csv(\"./data/main_dataset/filled_pdb_uniprot_db.csv\", index=False)\n",
    "print(len(df2))\n",
    "df2 = df2.drop_duplicates()\n",
    "print(len(df2))\n",
    "\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  0    P00918\n",
       "1  0    P0A9D2\n",
       "2  0    P06241\n",
       "3  0    P0AEG4\n",
       "   1    P00644\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(df2.uniprot.str.split(\" \").to_list()).stack()\n",
    "df3.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('novozymes-prediction-Gl9CRTFV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b13dc4add9efa918e6bb920c50afa2240557655d90455391ab57f21c65447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
