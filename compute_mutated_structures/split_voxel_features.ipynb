{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split voxel features\n",
    "In order to speed up computing we computed the voxel features by batch\n",
    "\n",
    "We now need to split those and add the infos the the dataset\n",
    "\n",
    "We already have relaxed_wild_3D_path and relaxed_mutated_3D_path and now want voxel_direct_path and voxel_reversed_path as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_INPUT = \"../data/main_dataset_creation/outputs/all_v3/dataset_with_all_features.csv\"\n",
    "SUBMISSION_INPUT = \"../data/main_dataset_creation/outputs/all_v3/submission_all_features_filled_nan.csv\"\n",
    "\n",
    "DATASET_OUTPUT = \"../data/main_dataset_creation/outputs/all_v3/dataset_with_voxel.csv\"\n",
    "SUBMISSION_OUTPUT = \"../data/main_dataset_creation/outputs/all_v3/submission_with_voxel.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATASET_INPUT)\n",
    "submission_df = pd.read_csv(SUBMISSION_INPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features(name: str, base_dir=\"./\", output_dir=\"./splitted_voxel_features/\"):\n",
    "    \"\"\"\n",
    "    function that creates a df identification infos and the avg prediction values of thermonet\n",
    "    (both direct and reversed)\n",
    "\n",
    "    returns:\n",
    "    a pandas.DataFrame which has the columns:\n",
    "    ['wild_path', 'position', 'mutated_path', 'direct_thermonet', 'reversed_thermonet']\n",
    "    \"\"\"\n",
    "    uniprot = name.split(\"_\")[0]\n",
    "    variant_path = base_dir+\"gends_input/\"+name+\"_variants.txt\"\n",
    "    base_features_path = base_dir+\"gends_output/\"+name\n",
    "\n",
    "    # first we get the variant list in a df, in order to know to which mutations each value corresponds to\n",
    "    variant_df = pd.read_csv(variant_path,\n",
    "                             names=[\"wild_path\", \"position\", \"mutated_path\"],\n",
    "                             sep=' ')\n",
    "    variant_df.position = variant_df.position.apply(lambda x: x-1).astype(int)\n",
    "    variant_df = pd.concat([variant_df, pd.DataFrame(columns=[\"direct_voxel_path\",\n",
    "                                                              \"reversed_voxel_path\"])],\n",
    "                           axis=1)\n",
    "\n",
    "    # then we load the features from the gends outputs\n",
    "    try:\n",
    "        direct_features = np.load(\n",
    "            base_features_path+\"_stacked_16_1_direct.npy\")\n",
    "        reversed_features = np.load(\n",
    "            base_features_path+\"_stacked_16_1_reversed.npy\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception raised for {name}, {base_features_path}: {e}\")\n",
    "        print(f\"not adding features for {name}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if len(variant_df) != direct_features.shape[0]:\n",
    "        print(\n",
    "            f\"ERROR for {name}, {base_features_path}, len(variant_df) != direct_features.shape[0]\")\n",
    "    for i, row in variant_df.iterrows():\n",
    "        # ./compute_mutated_structures/relaxed_pdb/P03050_relaxed/P03050_relaxed_P8A_relaxed.pdb\n",
    "        # => P8A\n",
    "        mutated_path = row[\"mutated_path\"]\n",
    "        try:\n",
    "            result = re.search('_relaxed_(.*)_relaxed', mutated_path)\n",
    "            mutation_code = result.group(1)\n",
    "        except:\n",
    "            # case where we have: compute_mutated_structures/relaxed_pdb/AF70_alphafold/K212__unrelaxed_rank_1_model_3_relaxed.pdb\n",
    "            result = re.search('_alphafold/(.*)_unrelaxed', mutated_path)\n",
    "            mutation_code = result.group(1)\n",
    "\n",
    "        direct = direct_features[i]\n",
    "        reversed = reversed_features[i]\n",
    "        direct_path = output_dir+f\"{uniprot}_{mutation_code}_direct\"\n",
    "        reversed_path = output_dir+f\"{uniprot}_{mutation_code}_reversed\"\n",
    "\n",
    "        variant_df.loc[i, \"direct_voxel_path\"] = direct_path\n",
    "        variant_df.loc[i, \"reversed_voxel_path\"] = reversed_path\n",
    "        np.save(direct_path, direct)\n",
    "        np.save(reversed_path, reversed)\n",
    "\n",
    "    return variant_df\n",
    "\n",
    "\n",
    "def update_main_df(row, main_df: pd.DataFrame):\n",
    "    # we get mutated_path as a unique protein+mutation identifier\n",
    "    # but multiple record could have the same mutation on the same protein\n",
    "    # for example same mutation at different pH\n",
    "\n",
    "    mutated_path = row[\"mutated_path\"]\n",
    "    main_df.loc[\n",
    "        (main_df.relaxed_mutated_3D_path.eq(mutated_path)),\n",
    "        \"direct_voxel_path\"\n",
    "    ] = row[\"direct_voxel_path\"]\n",
    "    main_df.loc[\n",
    "        (main_df.relaxed_mutated_3D_path.eq(mutated_path)),\n",
    "        \"reversed_voxel_path\"\n",
    "    ] = row[\"reversed_voxel_path\"]\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def split_voxel_features(main_df: pd.DataFrame, base_dir=\"./\"):\n",
    "    main_df = pd.concat([main_df, pd.DataFrame(columns=[\"direct_voxel_path\",\n",
    "                                                        \"reversed_voxel_path\"])],\n",
    "                        axis=1)\n",
    "\n",
    "    all_variants = glob(\n",
    "        f\"{base_dir}gends_input/*_variants.txt\")\n",
    "    all_names = [v.split('/')[-1].split(\"_variants.txt\")[0]\n",
    "                 for v in all_variants]\n",
    "    for name in all_names:\n",
    "        variant_df = split_features(\n",
    "            name, base_dir=base_dir)\n",
    "        variant_df.apply(lambda row: update_main_df(row, main_df), axis=1)\n",
    "\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised for O60885_260, ./gends_output/O60885_260: [Errno 2] No such file or directory: './gends_output/O60885_260_stacked_16_1_direct.npy'\n",
      "not adding features for O60885_260\n",
      "Exception raised for P02751_70, ./gends_output/P02751_70: [Errno 2] No such file or directory: './gends_output/P02751_70_stacked_16_1_direct.npy'\n",
      "not adding features for P02751_70\n",
      "Exception raised for O60885_260, ./gends_output/O60885_260: [Errno 2] No such file or directory: './gends_output/O60885_260_stacked_16_1_direct.npy'\n",
      "not adding features for O60885_260\n",
      "Exception raised for P02751_70, ./gends_output/P02751_70: [Errno 2] No such file or directory: './gends_output/P02751_70_stacked_16_1_direct.npy'\n",
      "not adding features for P02751_70\n"
     ]
    }
   ],
   "source": [
    "train_df = split_voxel_features(train_df)\n",
    "submission_df = split_voxel_features(submission_df)\n",
    "train_df.to_csv(DATASET_OUTPUT, index=False)\n",
    "submission_df.to_csv(SUBMISSION_OUTPUT, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATASET_OUTPUT)\n",
    "submission_df = pd.read_csv(SUBMISSION_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~(train_df.direct_voxel_path.isna())]\n",
    "train_df.to_csv(DATASET_OUTPUT, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(DATASET_OUTPUT)\n",
    "train_df.direct_voxel_path.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K212_'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = \"compute_mutated_structures/relaxed_pdb/AF70_alphafold/K212__unrelaxed_rank_1_model_3_relaxed.pdb\"\n",
    "result = re.search('_alphafold/(.*)_unrelaxed', s)\n",
    "result.group(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
