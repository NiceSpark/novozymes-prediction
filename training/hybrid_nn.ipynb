{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Neural Network for Novozyme Prediction\n",
    "This NB allows you to test a NN model and get training infos\n",
    "The different parts are:\n",
    "- Simple train: train a NN based on the parameters in the config file\n",
    "- Compute learning curve: mse for a dataset of 100/1000/3000/all rows\n",
    "- Compute feature importance: use saved models and scalers to compute the avg mse with each features being randomized one at a time\n",
    "- Compute submission: use saved models and scalers to compute the avg mse for each row of the submission dataset\n",
    "\n",
    "Note: for hyper parameters optimization see the wandb_training.py script, in which we use wandb to sweep through a list of posssible hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from training_utils.file_utils import (open_json, write_json, save_submission,\n",
    "                                       log_kfold_training, log_learning_curve)\n",
    "from training_utils.models import HybridNN\n",
    "from training_utils.model_utils import *\n",
    "from training_utils.training import k_fold_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "150\n",
      "150\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "KEEP_MODELS = True\n",
    "\n",
    "SIMPLE_TRAIN = True\n",
    "COMPUTE_LEARNING_CURVE = False\n",
    "COMPUTE_FEATURE_IMPORTANCE = False\n",
    "COMPUTE_SUBMISSION = False\n",
    "\n",
    "USE_PDB_CHAIN_VOXEL_FOR_SUBMISSION = True # must be true from 0.08 to 0.42 for cnn only !\n",
    "\n",
    "TRAINING_DIR = \"outputs/hybrid_0/\"\n",
    "\n",
    "config = open_json(\"hybrid_nn_config.json\")\n",
    "features_dict = open_json(\n",
    "    f\"{config['dataset_dir']}/{config['features_name']}.json\")\n",
    "features, features_infos = compute_feature_list(config, features_dict)\n",
    "device = get_device(config)\n",
    "log_name = config[\"model_type\"]\n",
    "\n",
    "print(len(features))\n",
    "print(len(features_infos[\"direct_features\"]))\n",
    "print(config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8568 data\n",
      "no valid ksplit path given, doing ksplit without groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if SIMPLE_TRAIN:\n",
    "    df = load_dataset(config, features, rm_nan=True)\n",
    "\n",
    "    all_training_results = {\"simple_train\": [],\n",
    "                            \"total_training_time\": 0}\n",
    "\n",
    "    # add protein_index to the dataset and get ksplit:\n",
    "    df = split_dataset(df, config)\n",
    "    # training\n",
    "    training_results = k_fold_training(\n",
    "        df, config, features, features_infos, device, keep_models=KEEP_MODELS)\n",
    "\n",
    "    # add training results to all the other ones\n",
    "    all_training_results[\"simple_train\"] = training_results\n",
    "    \n",
    "    # save results to output\n",
    "    model = HybridNN(\n",
    "        len(features_infos[\"direct_features\"]), config)\n",
    "    model_structure = str(model).replace(\n",
    "        '(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_kfold_training(\n",
    "        log_name, all_training_results, config, features, model_structure)\n",
    "\n",
    "    if KEEP_MODELS:\n",
    "        move_models_and_scalers(dir_path)\n",
    "\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_LEARNING_CURVE:\n",
    "    df = load_dataset(config, features)\n",
    "\n",
    "    # plot the learning curve of the model\n",
    "    # ie. the avg mse when df has 10, 100, 1000 elements\n",
    "    # len(df) = 5k\n",
    "    num_rows = [100, 1000, 3000, len(df)]\n",
    "    all_training_results = {\"training_by_num_rows\": [],\n",
    "                            \"learning_curve\": {\"num_rows\": num_rows,\n",
    "                                               \"train_mse\": [],\n",
    "                                               \"test_mse\": []\n",
    "                                               },\n",
    "                            \"total_training_time\": 0\n",
    "                            }\n",
    "\n",
    "    t0 = time.time()\n",
    "    for n in num_rows:\n",
    "        print(f\"training on {n} rows from the dataset\")\n",
    "        df_n_rows = df.sample(n)\n",
    "        # add protein_index to the dataset and get ksplit:\n",
    "        df_n_rows, ksplit = split_dataset(df_n_rows, config)\n",
    "        training_results = k_fold_training(\n",
    "            df_n_rows, ksplit, config, features, features_infos, device)\n",
    "\n",
    "        # add training results to all the other ones\n",
    "        all_training_results[\"training_by_num_rows\"].append(training_results)\n",
    "        # compute avg_mse and time\n",
    "        train_mse = sum(x[\"train_mse\"]\n",
    "                        for x in training_results)/config[\"kfold\"]\n",
    "        test_mse = sum(x[\"test_mse\"]\n",
    "                       for x in training_results)/config[\"kfold\"]\n",
    "        training_time = sum(x[\"time\"] for x in training_results)\n",
    "\n",
    "        # update result variables\n",
    "        all_training_results[\"total_training_time\"] += training_time\n",
    "        all_training_results[\"learning_curve\"][\"train_mse\"].append(train_mse)\n",
    "        all_training_results[\"learning_curve\"][\"test_mse\"].append(test_mse)\n",
    "\n",
    "    total_time = time.time()-t0\n",
    "    print(f\"total_training_time= {all_training_results['total_training_time']:.2f}, {total_time= :.2f}, \\\n",
    "        training_time: {(all_training_results['total_training_time']/total_time)*100:.2f}% of total time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_LEARNING_CURVE:\n",
    "    # save results to output\n",
    "    model = HybridNN(\n",
    "        len(features_infos[\"direct_features\"]), config)\n",
    "    model_structure = str(model).replace('(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_learning_curve(\n",
    "        log_name, all_training_results, config, features, model_structure)\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "cf model_visualization NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_SUBMISSION:\n",
    "    dir_path = TRAINING_DIR\n",
    "    submission_df = pd.read_csv(\n",
    "        f\"{config['dataset_dir']}/submission_6_12.csv\")\n",
    "\n",
    "    if USE_PDB_CHAIN_VOXEL_FOR_SUBMISSION:\n",
    "        submission_df[\"direct_voxel_features\"] = submission_df[\"kaggle_voxel_path\"].apply(\n",
    "            np.load)\n",
    "    else:\n",
    "        # load voxel directly in df\n",
    "        submission_df[\"direct_voxel_features\"] = submission_df[\"direct_voxel_path\"].apply(\n",
    "            np.load)\n",
    "    ddG_results = []\n",
    "    dTm_results = []\n",
    "\n",
    "    model_list, all_scalers = load_models_and_scalers(dir_path)\n",
    "\n",
    "    for k in range(len(model_list)):\n",
    "        model = model_list[k]  # model result from the training\n",
    "        dataset_train_scaler = all_scalers[\"X\"][k]  # scaler from training\n",
    "        ddG_scaler = all_scalers[\"ddG\"][k]\n",
    "        dTm_scaler = all_scalers[\"dTm\"][k]\n",
    "\n",
    "        X_voxel_features, X_features, _, _ = prepare_eval_data(submission_df, config,\n",
    "                                                            features, features_infos,\n",
    "                                                            dataset_train_scaler,\n",
    "                                                            submission=True)\n",
    "\n",
    "        # Evaluate this model:\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            X_voxel_features = X_voxel_features.to(device)\n",
    "            X_features = X_features.to(device)\n",
    "            y_test_ddG, y_test_dTm = model(X_voxel_features, X_features)\n",
    "            y_test_ddG = y_test_ddG.cpu().detach().numpy()\n",
    "            y_test_dTm = y_test_dTm.cpu().detach().numpy()\n",
    "            # we scale back the result to get to actual dTm and ddG values\n",
    "            y_test_ddG = ddG_scaler.inverse_transform(y_test_ddG)\n",
    "            y_test_dTm = dTm_scaler.inverse_transform(y_test_dTm)\n",
    "            # we append to the list of results\n",
    "            ddG_results.append(y_test_ddG)\n",
    "            dTm_results.append(y_test_dTm)\n",
    "\n",
    "    submission = pd.DataFrame(columns=[\"seq_id\", \"tm\"])\n",
    "    submission[\"seq_id\"] = submission_df[\"seq_id\"]\n",
    "    submission[\"tm\"] = np.mean(np.array(dTm_results), axis=0)\n",
    "    # submission[\"ddg\"] = np.mean(np.array(ddG_results), axis=0)\n",
    "    print(submission.head())\n",
    "    save_path = save_submission(submission, TRAINING_DIR)\n",
    "    print(f\"{save_path=}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
