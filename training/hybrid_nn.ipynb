{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Neural Network for Novozyme Prediction\n",
    "This NB allows you to test a NN model and get training infos\n",
    "The different parts are:\n",
    "- Simple train: train a NN based on the parameters in the config file\n",
    "- Compute learning curve: mse for a dataset of 100/1000/3000/all rows\n",
    "- Compute feature importance: use saved models and scalers to compute the avg mse with each features being randomized one at a time\n",
    "- Compute submission: use saved models and scalers to compute the avg mse for each row of the submission dataset\n",
    "\n",
    "Note: for hyper parameters optimization see the wandb_training.py script, in which we use wandb to sweep through a list of posssible hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from training_utils.file_utils import (open_json, write_json, save_submission,\n",
    "                                       log_kfold_training, log_learning_curve)\n",
    "from training_utils.models import HybridNN\n",
    "from training_utils.model_utils import *\n",
    "from training_utils.training import k_fold_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "150\n",
      "150\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "KEEP_MODELS = True\n",
    "\n",
    "SIMPLE_TRAIN = True\n",
    "COMPUTE_LEARNING_CURVE = False\n",
    "COMPUTE_FEATURE_IMPORTANCE = False\n",
    "COMPUTE_SUBMISSION = True\n",
    "\n",
    "USE_KAGGLE_VOXEL_FOR_SUBMISSION = False\n",
    "GET_FEATURES_BY_CORRELATION = False\n",
    "\n",
    "TRAINING_DIR = \"outputs/hybrid_2/\"\n",
    "\n",
    "config = open_json(\"hybrid_nn_config.json\")\n",
    "features_dict = open_json(\n",
    "    f\"{config['dataset_dir']}/{config['features_name']}.json\")\n",
    "features, features_infos = compute_feature_list(config, features_dict)\n",
    "device = get_device(config)\n",
    "log_name = config[\"model_type\"]\n",
    "\n",
    "print(len(features))\n",
    "print(len(features_infos[\"direct_features\"]))\n",
    "print(config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMPLE_TRAIN:\n",
    "    df = load_dataset(config, features, rm_nan=True)\n",
    "\n",
    "    all_training_results = {\"simple_train\": [],\n",
    "                            \"total_training_time\": 0}\n",
    "\n",
    "    # add protein_index to the dataset and get ksplit:\n",
    "    df = split_dataset(df, config)\n",
    "    # training\n",
    "    training_results = k_fold_training(\n",
    "        df, config, features, features_infos, device, keep_models=KEEP_MODELS)\n",
    "\n",
    "    # add training results to all the other ones\n",
    "    all_training_results[\"simple_train\"] = training_results\n",
    "    \n",
    "    # save results to output\n",
    "    model = HybridNN(\n",
    "        len(features_infos[\"direct_features\"]), config)\n",
    "    model_structure = str(model).replace(\n",
    "        '(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_kfold_training(\n",
    "        log_name, all_training_results, config, features, model_structure)\n",
    "\n",
    "    if KEEP_MODELS:\n",
    "        move_models_and_scalers(dir_path)\n",
    "\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find worst samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if SIMPLE_TRAIN:\n",
    "    WORST_SAMPLES_DIFF_THRESHOLD = 7.5\n",
    "    worst_of_the_worst = []\n",
    "    # WORST_SAMPLES_DIFF_THRESHOLD = 1\n",
    "    for res in all_training_results.get(\"simple_train\"):\n",
    "        worst_samples = res.get(\"worst_samples\")\n",
    "        worst_of_the_worst += [record for record in worst_samples if record[\"diff\"]\n",
    "                               > WORST_SAMPLES_DIFF_THRESHOLD]\n",
    "\n",
    "    print(len(worst_of_the_worst))\n",
    "    write_json(\"worst_samples.json\", worst_of_the_worst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_LEARNING_CURVE:\n",
    "    df = load_dataset(config, features)\n",
    "\n",
    "    # plot the learning curve of the model\n",
    "    # ie. the avg mse when df has 10, 100, 1000 elements\n",
    "    # len(df) = 5k\n",
    "    num_rows = [100, 1000, 3000, len(df)]\n",
    "    all_training_results = {\"training_by_num_rows\": [],\n",
    "                            \"learning_curve\": {\"num_rows\": num_rows,\n",
    "                                               \"train_mse\": [],\n",
    "                                               \"test_mse\": []\n",
    "                                               },\n",
    "                            \"total_training_time\": 0\n",
    "                            }\n",
    "\n",
    "    t0 = time.time()\n",
    "    for n in num_rows:\n",
    "        print(f\"training on {n} rows from the dataset\")\n",
    "        df_n_rows = df.sample(n)\n",
    "        # add protein_index to the dataset and get ksplit:\n",
    "        df_n_rows, ksplit = split_dataset(df_n_rows, config)\n",
    "        training_results = k_fold_training(\n",
    "            df_n_rows, ksplit, config, features, features_infos, device)\n",
    "\n",
    "        # add training results to all the other ones\n",
    "        all_training_results[\"training_by_num_rows\"].append(training_results)\n",
    "        # compute avg_mse and time\n",
    "        train_mse = sum(x[\"train_mse\"]\n",
    "                        for x in training_results)/config[\"kfold\"]\n",
    "        test_mse = sum(x[\"test_mse\"]\n",
    "                       for x in training_results)/config[\"kfold\"]\n",
    "        training_time = sum(x[\"time\"] for x in training_results)\n",
    "\n",
    "        # update result variables\n",
    "        all_training_results[\"total_training_time\"] += training_time\n",
    "        all_training_results[\"learning_curve\"][\"train_mse\"].append(train_mse)\n",
    "        all_training_results[\"learning_curve\"][\"test_mse\"].append(test_mse)\n",
    "\n",
    "    total_time = time.time()-t0\n",
    "    print(f\"total_training_time= {all_training_results['total_training_time']:.2f}, {total_time= :.2f}, \\\n",
    "        training_time: {(all_training_results['total_training_time']/total_time)*100:.2f}% of total time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_LEARNING_CURVE:\n",
    "    # save results to output\n",
    "    model = HybridNN(\n",
    "        len(features_infos[\"direct_features\"]), config)\n",
    "    model_structure = str(model).replace('(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_learning_curve(\n",
    "        log_name, all_training_results, config, features, model_structure)\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_FEATURE_IMPORTANCE:\n",
    "    \"\"\"you need to provide a dir_path to a \"\"\"\n",
    "    # all_training_results[feature1]: result obtained when df[feature1] = random\n",
    "    all_training_results = {}\n",
    "    dir_path = TRAINING_DIR\n",
    "    print(dir_path)\n",
    "\n",
    "\n",
    "    df = load_dataset(config, features, rm_nan=True)\n",
    "    model_list, scaler_list = load_models_and_scalers(dir_path)\n",
    "    training_results = open_json(dir_path+\"results.json\")\n",
    "    # 1. we already trained k models using all features:\n",
    "    df = split_dataset(df, config)\n",
    "\n",
    "    # compute mse (with no feature randomized)\n",
    "    none_random_train_mse = sum(x['train_mse_over_time'][x[\"best_epoch\"]]\n",
    "                                for x in training_results[\"simple_train\"])/config['kfold']\n",
    "    none_random_test_mse = sum(x['best_test_mse']\n",
    "                            for x in training_results[\"simple_train\"])/config['kfold']\n",
    "    all_training_results[\"none_randomized\"] = {\n",
    "        \"train_mse\": none_random_train_mse, \"test_mse\": none_random_test_mse}\n",
    "\n",
    "    # 2. for each feature we take the df, randomize the feature column, then compute the new mse score with each model\n",
    "    print(\"now testing with random features\")\n",
    "    for feature in tqdm.tqdm(features):\n",
    "        # create a copy of the dataset with the feature column being random\n",
    "        df_random = copy.deepcopy(df)\n",
    "        df_random[feature] = np.random.uniform(\n",
    "            low=df[feature].min(), high=df[feature].max(), size=df_random.shape[0])\n",
    "        training_results = []\n",
    "\n",
    "        # evaluate all model on this new dataset\n",
    "        for k in range(config[\"kfold\"]):\n",
    "            # we get the same ksplit as during training as we already added the kfold column to df\n",
    "            train = list(range(config[\"kfold\"]))\n",
    "            test = [train.pop(k)]\n",
    "            df_train = df_random[df_random[\"kfold\"].isin(train)]\n",
    "            df_test = df_random[df_random[\"kfold\"].isin(test)]\n",
    "            model = model_list[k]  # model result from the training\n",
    "            dataset_train_scaler = scaler_list[k]  # scaler from training\n",
    "\n",
    "            # we load the data for evaluation\n",
    "            X_train_voxel, X_train_features, y_train = prepare_eval_data(\n",
    "                df_train, config, features, features_infos, dataset_train_scaler)\n",
    "            X_test_voxel, X_test_features, y_test = prepare_eval_data(\n",
    "                df_test, config, features, features_infos, dataset_train_scaler)\n",
    "\n",
    "            # Evaluate this model:\n",
    "            model.eval()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                train_mse, _ = evaluate_model(\n",
    "                    X_train_voxel, X_train_features, y_train, model, device)\n",
    "                test_mse, _ = evaluate_model(\n",
    "                    X_test_voxel, X_test_features, y_test, model, device)\n",
    "                # print(f\"MSE obtained for kfold {k}: {mse}\")\n",
    "                results = {\n",
    "                    \"train_mse\": train_mse,\n",
    "                    \"test_mse\": test_mse\n",
    "                }\n",
    "                training_results.append(results)\n",
    "\n",
    "        # compute avg_mse\n",
    "        train_mse = sum(x['train_mse']\n",
    "                        for x in training_results)/config['kfold']\n",
    "        test_mse = sum(x['test_mse']\n",
    "                    for x in training_results)/config['kfold']\n",
    "        # add training results to all the other ones\n",
    "        all_training_results[feature] = {\n",
    "            \"train_mse\": train_mse, \"test_mse\": test_mse,\n",
    "            \"delta_train_mse\": train_mse-none_random_train_mse,\n",
    "            \"delta_test_mse\": test_mse-none_random_test_mse,\n",
    "        }\n",
    "\n",
    "    # rank results\n",
    "    ranked_delta_test_mse = [[x, all_training_results[x].get(\n",
    "        \"delta_test_mse\", 0)] for x in all_training_results.keys()]\n",
    "    ranked_delta_test_mse.sort(key=lambda x: x[1], reverse=True)\n",
    "    ranked_delta_test_mse = {x[0]: x[1] for x in ranked_delta_test_mse}\n",
    "    write_json(f\"{dir_path}{log_name}_feature_importance.json\",\n",
    "            {\"ranked_delta_test_mse\": ranked_delta_test_mse,\n",
    "                \"all_training_results\": all_training_results})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_FEATURE_IMPORTANCE:\n",
    "    all_feature_importance_by_category = {}\n",
    "    dir_path = TRAINING_DIR\n",
    "    print(dir_path)\n",
    "    ranked_delta_test_mse = open_json(\n",
    "        f\"{dir_path}{log_name}_feature_importance.json\")[\"ranked_delta_test_mse\"]\n",
    "    feature_importance_by_category = {}\n",
    "    for k, sublist in features_dict.items():\n",
    "        total = 0\n",
    "        cat = {}\n",
    "        for feature in sublist:\n",
    "            if feature in ranked_delta_test_mse:\n",
    "                value = ranked_delta_test_mse[feature]\n",
    "                total += value\n",
    "                cat[feature] = value \n",
    "        cat[\"sum_importances\"] = total\n",
    "        feature_importance_by_category[k] = cat\n",
    "    write_json(f\"{dir_path}feature_importance_by_category.json\",\n",
    "            feature_importance_by_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seq_id        tm\n",
      "0   31390 -0.197909\n",
      "1   31391 -0.208402\n",
      "2   31392 -0.169326\n",
      "3   31393 -0.359036\n",
      "4   31394 -0.480545\n",
      "save_path='./submissions/hybrid_2022-12-03_20-26-36/hybrid_2022-12-03_20-26-36.csv'\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_SUBMISSION:\n",
    "    dir_path = TRAINING_DIR\n",
    "    submission_df = pd.read_csv(\n",
    "        f\"{config['dataset_dir']}/submission_with_voxel_from_kaggle_filled_nan.csv\")\n",
    "\n",
    "    if USE_KAGGLE_VOXEL_FOR_SUBMISSION:\n",
    "        submission_df[\"direct_voxel_features\"] = submission_df[\"kaggle_voxel_path\"].apply(\n",
    "            np.load)\n",
    "    else:\n",
    "        # load voxel directly in df\n",
    "        submission_df[\"direct_voxel_features\"] = submission_df[\"direct_voxel_path\"].apply(\n",
    "            np.load)\n",
    "    results = []\n",
    "\n",
    "    model_list, scaler_list = load_models_and_scalers(dir_path)\n",
    "\n",
    "    for k in range(len(model_list)):\n",
    "        model = model_list[k]  # model result from the training\n",
    "        dataset_train_scaler = scaler_list[k]  # scaler from training\n",
    "        X_voxel_features, X_features, _ = prepare_eval_data(submission_df, config,\n",
    "                                                            features, features_infos,\n",
    "                                                            dataset_train_scaler,\n",
    "                                                            submission=True)\n",
    "\n",
    "        # Evaluate this model:\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            X_voxel_features = X_voxel_features.to(device)\n",
    "            X_features = X_features.to(device)\n",
    "            y_test = model(X_voxel_features, X_features)\n",
    "            results.append(y_test.cpu().detach().numpy())\n",
    "\n",
    "    submission = pd.DataFrame(columns=[\"seq_id\", \"tm\"])\n",
    "    submission[\"seq_id\"] = submission_df[\"seq_id\"]\n",
    "    submission[\"tm\"] = np.mean(np.array(results), axis=0)\n",
    "    print(submission.head())\n",
    "    save_path = save_submission(submission, TRAINING_DIR)\n",
    "    print(f\"{save_path=}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
