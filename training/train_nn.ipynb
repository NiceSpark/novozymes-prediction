{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "\n",
    "from training_utils.file_utils import (open_json, write_json, save_submission,\n",
    "                                       log_whole_dataset_training, log_kfold_training, log_learning_curve)\n",
    "from training_utils.models import SimpleNN\n",
    "from training_utils.model_utils import *\n",
    "from training_utils.training import train_model, k_fold_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = open_json(\"simple_nn_config.json\")\n",
    "DIR_PATH = config[\"dataset_dir\"]\n",
    "features_dict = open_json(f\"{DIR_PATH}/features.json\")\n",
    "features, features_infos = compute_feature_list(config, features_dict)\n",
    "\n",
    "LOG_NAME = \"SimpleNN\"\n",
    "\n",
    "SIMPLE_TRAIN = False\n",
    "WANDB_SWEEP = True\n",
    "COMPUTE_LEARNING_CURVE = False\n",
    "COMPUTE_FEATURE_IMPORTANCE = False\n",
    "TRAIN_WHOLE_DATASET = False\n",
    "COMPUTE_SUBMISSION = False\n",
    "\n",
    "WANDB_SWEEP_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and config[\"use_cuda\"]:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb \n",
    "setup for hyper parameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: bpcwer5l\n",
      "Sweep URL: https://wandb.ai/thomasg_42/test/sweeps/bpcwer5l\n",
      "set up new sweep sweep_id='bpcwer5l'\n"
     ]
    }
   ],
   "source": [
    "if WANDB_SWEEP:\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "    sweep_config = open_json(\"wandb_sweep_config.json\")\n",
    "    if WANDB_SWEEP_ID is None:\n",
    "        sweep_id = wandb.sweep(sweep_config, project=\"test\")\n",
    "        print(f\"set up new sweep {sweep_id=}\")\n",
    "    else:\n",
    "        sweep_id = WANDB_SWEEP_ID\n",
    "        print(f\"using given {sweep_id=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config):\n",
    "    df = pd.read_csv(f\"{DIR_PATH}/{config['dataset_name']}.csv\")\n",
    "\n",
    "    for feature in features:\n",
    "        # remove line with nan values for selected features\n",
    "        df = df[~(df[feature].isna())]\n",
    "\n",
    "    # remove bad uniprot\n",
    "    df = df[~(df[\"uniprot\"].isin(config[\"bad_uniprot\"]))]\n",
    "\n",
    "    # apply max protein length\n",
    "    df = df[df.length.le(config[\"max_protein_length\"])]\n",
    "\n",
    "    print(f\"training on {len(df)} data\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 5866 data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494f3e6273e2452caf8c253b69847af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run hp18d66m errored: KeyError('target')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63450c2d8eab41e199e1e273e9ba2362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run mbfpuaqo errored: KeyError('target')\n"
     ]
    }
   ],
   "source": [
    "if WANDB_SWEEP:\n",
    "    global_config = config.copy()\n",
    "    main_df = load_dataset(global_config)\n",
    "    def run():\n",
    "        # add protein_index to the dataset and get ksplit:\n",
    "        df, ksplit = split_dataset(main_df, global_config)\n",
    "        training_results, _, _ = k_fold_training(\n",
    "            df, ksplit, global_config, features, \n",
    "            features_infos, device, wandb_active=True)\n",
    "    try:\n",
    "        wandb.agent(sweep_id, run)\n",
    "        wandb.finish()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMPLE_TRAIN:\n",
    "    df = load_dataset(config)\n",
    "\n",
    "    all_training_results = {\"simple_train\": [],\n",
    "                            \"total_training_time\": 0}\n",
    "    print(f\"{type(all_training_results)=}\")\n",
    "\n",
    "    # add protein_index to the dataset and get ksplit:\n",
    "    df, ksplit = split_dataset(df, config)\n",
    "    training_results, _, _ = k_fold_training(\n",
    "        df, ksplit, config, features, features_infos, device)\n",
    "\n",
    "    # add training results to all the other ones\n",
    "    all_training_results[\"simple_train\"] = training_results\n",
    "    print(f\"{type(all_training_results)=}\")\n",
    "    # compute avg_mse and time\n",
    "    train_mse = sum(x[\"train_mse\"]\n",
    "                    for x in training_results)/config[\"kfold\"]\n",
    "    test_mse = sum(x[\"test_mse\"]\n",
    "                   for x in training_results)/config[\"kfold\"]\n",
    "    training_time = sum(x[\"time\"] for x in training_results)\n",
    "\n",
    "    # save results to output\n",
    "    model = SimpleNN(\n",
    "        len(features_infos[\"direct_features\"]), config[\"model_config\"])\n",
    "    model_structure = str(model).replace('(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_kfold_training(\n",
    "        LOG_NAME, all_training_results, config, features, model_structure)\n",
    "    print(f\"{type(all_training_results)=}\")\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find worst samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMPLE_TRAIN:\n",
    "    WORST_SAMPLES_DIFF_THRESHOLD = 10\n",
    "    worst_of_the_worst = []\n",
    "    print(f\"{type(all_training_results)=}\")\n",
    "    # WORST_SAMPLES_DIFF_THRESHOLD = 1\n",
    "    for res in all_training_results.get(\"simple_train\"):\n",
    "        worst_samples = res.get(\"worst_samples\")\n",
    "        worst_of_the_worst += [record for record in worst_samples if record[\"diff\"]\n",
    "                               > WORST_SAMPLES_DIFF_THRESHOLD]\n",
    "\n",
    "    print(len(worst_of_the_worst))\n",
    "    write_json(\"worst_samples.json\", worst_of_the_worst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_LEARNING_CURVE:\n",
    "    df = load_dataset(config)\n",
    "    # plot the learning curve of the model\n",
    "    # ie. the avg mse when df has 10, 100, 1000 elements\n",
    "    # len(df) = 5k\n",
    "    num_rows = [100, 1000, 3000, len(df)]\n",
    "    all_training_results = {\"training_by_num_rows\": [],\n",
    "                            \"learning_curve\": {\"num_rows\": num_rows,\n",
    "                                               \"train_mse\": [],\n",
    "                                               \"test_mse\": []\n",
    "                                               },\n",
    "                            \"total_training_time\": 0\n",
    "                            }\n",
    "\n",
    "    t0 = time.time()\n",
    "    for n in num_rows:\n",
    "        print(f\"training on {n} rows from the dataset\")\n",
    "        df_n_rows = df.sample(n)\n",
    "        # add protein_index to the dataset and get ksplit:\n",
    "        df_n_rows, ksplit = split_dataset(df_n_rows, config)\n",
    "        training_results, _, _ = k_fold_training(\n",
    "            df_n_rows, ksplit, config, features, device)\n",
    "\n",
    "        # add training results to all the other ones\n",
    "        all_training_results[\"training_by_num_rows\"].append(training_results)\n",
    "        # compute avg_mse and time\n",
    "        train_mse = sum(x[\"train_mse\"]\n",
    "                        for x in training_results)/config[\"kfold\"]\n",
    "        test_mse = sum(x[\"test_mse\"]\n",
    "                       for x in training_results)/config[\"kfold\"]\n",
    "        training_time = sum(x[\"time\"] for x in training_results)\n",
    "\n",
    "        # update result variables\n",
    "        all_training_results[\"total_training_time\"] += training_time\n",
    "        all_training_results[\"learning_curve\"][\"train_mse\"].append(train_mse)\n",
    "        all_training_results[\"learning_curve\"][\"test_mse\"].append(test_mse)\n",
    "\n",
    "    total_time = time.time()-t0\n",
    "    print(f\"total_training_time= {all_training_results['total_training_time']:.2f}, {total_time= :.2f}, \\\n",
    "        training_time: {(all_training_results['total_training_time']/total_time)*100:.2f}% of total time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_LEARNING_CURVE:\n",
    "    # save results to output\n",
    "    model = SimpleNN(\n",
    "        len(features_infos[\"direct_features\"]), config[\"model_config\"])\n",
    "    model_structure = str(model).replace('(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_kfold_training(\n",
    "        LOG_NAME, all_training_results, config, features, model_structure)\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_FEATURE_IMPORTANCE:\n",
    "    # all_training_results[feature1]: result obtained when df[feature1] = random\n",
    "    all_training_results = {}\n",
    "\n",
    "    df = load_dataset(config)\n",
    "\n",
    "    # 1. we train k-fold models using all features:\n",
    "    df, ksplit = split_dataset(df, config)\n",
    "    ksplit_list = list(ksplit)\n",
    "    training_results, model_list, scaler_list = k_fold_training(\n",
    "        df, iter(ksplit_list), config, features, device, keep_models=True)\n",
    "\n",
    "    # compute mse (with no feature randomized)\n",
    "    none_random_train_mse = sum(x['train_mse']\n",
    "                                for x in training_results)/config['k-fold']\n",
    "    none_random_test_mse = sum(x['test_mse']\n",
    "                               for x in training_results)/config['k-fold']\n",
    "    all_training_results[\"none_randomized\"] = {\n",
    "        \"train_mse\": none_random_train_mse, \"test_mse\": none_random_test_mse}\n",
    "\n",
    "    # 2. for each feature we take the df, randomize the feature column, then compute the new mse score with each model\n",
    "    for feature in features:\n",
    "        # create a copy of the dataset with the feature column being random\n",
    "        df_random = copy.deepcopy(df)\n",
    "        df_random[feature] = np.random.randint(-1000, 1000, df_random.shape[0])\n",
    "        training_results = []\n",
    "\n",
    "        # evaluate all model on this new dataset\n",
    "        ksplit = iter(ksplit_list)\n",
    "        for k in range(config[\"kfold\"]):\n",
    "            # get the same ksplit as during training\n",
    "            train, test = next(ksplit)\n",
    "            model = model_list[k]  # model result from the training\n",
    "            dataset_train_scaler = scaler_list[k]  # scaler from training\n",
    "\n",
    "            # we load the data for evaluation\n",
    "            df_train = df_random[df_random[\"protein_index\"].isin(train)]\n",
    "            df_test = df_random[df_random[\"protein_index\"].isin(test)]\n",
    "            X_train, y_train = prepare_eval_data(\n",
    "                df_train, config, features, dataset_train_scaler)\n",
    "            X_test, y_test = prepare_eval_data(\n",
    "                df_test, config, features, dataset_train_scaler)\n",
    "\n",
    "            # Evaluate this model:\n",
    "            model.eval()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                train_mse = evaluate_model(X_train, y_train, model, device)\n",
    "                test_mse = evaluate_model(X_test, y_test, model, device)\n",
    "                # print(f\"MSE obtained for k-fold {k}: {mse}\")\n",
    "                results = {\n",
    "                    \"train_mse\": train_mse,\n",
    "                    \"test_mse\": test_mse\n",
    "                }\n",
    "                training_results.append(results)\n",
    "\n",
    "        # compute avg_mse\n",
    "        train_mse = sum(x['train_mse']\n",
    "                        for x in training_results)/config['k-fold']\n",
    "        test_mse = sum(x['test_mse']\n",
    "                       for x in training_results)/config['k-fold']\n",
    "        # add training results to all the other ones\n",
    "        all_training_results[feature] = {\n",
    "            \"train_mse\": train_mse, \"test_mse\": test_mse,\n",
    "            \"delta_train_mse\": train_mse-none_random_train_mse,\n",
    "            \"delta_test_mse\": test_mse-none_random_test_mse,\n",
    "        }\n",
    "\n",
    "    # rank results\n",
    "    ranked_delta_test_mse = [[x, all_training_results[x].get(\n",
    "        \"delta_test_mse\", 0)] for x in all_training_results.keys()]\n",
    "    ranked_delta_test_mse.sort(key=lambda x: x[1], reverse=True)\n",
    "    ranked_delta_test_mse = {x[0]: x[1] for x in ranked_delta_test_mse}\n",
    "    write_json(f\"./outputs/{LOG_NAME}_feature_importance.json\",\n",
    "               {\"ranked_delta_test_mse\": ranked_delta_test_mse,\n",
    "                \"all_training_results\": all_training_results})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop run all\n",
    "assert False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_dataset_training(df, config, features, device):\n",
    "    \"\"\"\n",
    "    This function is to be sure to use all available data for submission\n",
    "    This end up seeing the test dataset from kaggle as the test set, without a cv split\n",
    "    This is not best practice for generalisation, \n",
    "    but we are trying to get the highest LB score afterall...\n",
    "    \"\"\"\n",
    "\n",
    "    training_results = []\n",
    "\n",
    "    # we load the data for training\n",
    "    dataset_train = prepare_train_data(df, config, features)\n",
    "    trainloader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                              batch_size=config[\"batch_size\"],\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=config[\"num_workers\"])\n",
    "\n",
    "    # we load the data for evaluation\n",
    "    X_train, y_train = prepare_eval_data(\n",
    "        df, config, features, dataset_train.scaler)\n",
    "\n",
    "    # Initialize a new Novozymes Model\n",
    "    model = SimpleNN(\n",
    "        len(features_infos[\"direct_features\"]), config[\"model_config\"])\n",
    "    model.to(torch.double)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train model:\n",
    "    model, loss_over_time, mse_over_time, _ = train_model(\n",
    "        model, config, optimizer, loss_function, trainloader, device)\n",
    "\n",
    "    # Evaluate this model:\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_mse = evaluate_model(X_train, y_train, model, device)\n",
    "        # print(f\"MSE obtained for k-fold {k}: {mse}\")\n",
    "        results = {\n",
    "            \"loss_over_time\": loss_over_time,\n",
    "            \"mse_over_time\": mse_over_time,\n",
    "            \"train_mse\": train_mse\n",
    "        }\n",
    "\n",
    "    # Process is complete.\n",
    "\n",
    "    return model, results, dataset_train.scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_WHOLE_DATASET:\n",
    "    df = pd.read_csv(f\"{DIR_PATH}/{config['dataset_name']}.csv\")\n",
    "    model, results, train_scaler = whole_dataset_training(\n",
    "        df, config, features, device)\n",
    "\n",
    "    # save results to output\n",
    "    model_structure = str(model).replace('(', '').replace(')', '').split('\\n')\n",
    "    dir_path = log_whole_dataset_training(LOG_NAME+\"_whole_dataset\", results,\n",
    "                                          config, features, model_structure)\n",
    "    print(f\"logged training in {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_WHOLE_DATASET and COMPUTE_SUBMISSION:\n",
    "    df_test = pd.read_csv(f\"{DIR_PATH}/processed_test.csv\")\n",
    "    X_test = df_test[features]\n",
    "    X_test = train_scaler.transform(X_test)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = model(X_test)\n",
    "    submission = pd.DataFrame(columns=[\"seq_id\", \"tm\"])\n",
    "    submission[\"seq_id\"] = df_test[\"seq_id\"]\n",
    "    submission[\"tm\"] = y_test.detach().numpy() * -1\n",
    "    print(submission.head())\n",
    "    save_path = save_submission(submission, LOG_NAME)\n",
    "    print(f\"{save_path=}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('novozymes-prediction-Gl9CRTFV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b13dc4add9efa918e6bb920c50afa2240557655d90455391ab57f21c65447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
