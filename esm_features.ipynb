{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from cuml import PCA\n",
    "from biopandas.pdb import PandasPdb\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from utils.alphafold import check_atom_coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer ESM Features\n",
    "\n",
    "In order to convert amino acid sequences aka proteins into meaningful features, we will use embeddings from SOTA protein transformer. We use Facebook\"s pretrained protein transformer ESM (Evolutionary Scale Modeling) with research paper here and GitHub here. Kaggleqrdl provided a starter notebook here. In version 15+, we also extract mutation probabilties and mutation entropy from ESM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"all_v2_2\"\n",
    "INPUT_DATASET = f\"./data/main_dataset_creation/outputs/{DATASET_NAME}/dataset_with_alphafold_paths.csv\"\n",
    "OUTPUT_DATASET = f\"./data/main_dataset_creation/outputs/{DATASET_NAME}/dataset_with_esm_features.csv\"\n",
    "MAX_CUDA_SEQ_LEN = 7000 # out of memory w/ the 3070 after this\n",
    "PCA_CT = 16  # random sample size per protein to fit PCA with\n",
    "SUBSET_DUPLICATES_NO_PH = [\"uniprot\", \"wild_aa\", \"mutation_position\",\n",
    "                           \"mutated_aa\", \"sequence\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/kaggleqrdl/esm-quick-start-lb237\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "token_map = {'L': 0, 'A': 1, 'G': 2, 'V': 3, 'S': 4, 'E': 5, 'R': 6, 'T': 7, 'I': 8, 'D': 9, 'P': 10,\n",
    "             'K': 11, 'Q': 12, 'N': 13, 'F': 14, 'Y': 15, 'M': 16, 'H': 17, 'W': 18, 'C': 19}\n",
    "t_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "t_model.eval()  # disables dropout for deterministic results\n",
    "print(\"loaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "We input each train and test wildtype into our transformer and extract the last hidden layers activations. For each protein, this has shape (1, len_protein_seq, 1280). We will save the full embeddings and the pooled embeddings for use later. Additionally we will save the MLM pretrain task amino acid prediction which indicates mutation probability and mutation entropy. This has shape (1, len_protein_seq, 33) but we extract to (len_protein_seq, 20) where 20 is number of common amino acids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking coherence between 477 pairs of sequence-atom(pdb) files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 32.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for ./data/main_dataset_creation/3D_structures/alphafold/P28335.pdb at position 22: C instead of S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:02, 40.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for ./data/main_dataset_creation/3D_structures/alphafold/P00749.pdb at position 140: P instead of L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "477it [00:14, 32.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 non coherent sequence-atom(pdb) pairs\n",
      "11262\n",
      "8734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_DATASET)\n",
    "df = check_atom_coherence(df)\n",
    "print(len(df))\n",
    "df.drop_duplicates(subset=SUBSET_DUPLICATES_NO_PH, inplace=True)\n",
    "print(len(df))\n",
    "# df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(all_sequences, embeddings, t_model, device):\n",
    "    # EXTRACT TRANSFORMER EMBEDDINGS FOR TRAIN AND TEST WILDTYPES\n",
    "    print(\"Extracting embeddings from proteins...\")\n",
    "    \n",
    "    all_seq_embed_pool = embeddings[\"all_seq_embed_pool\"]\n",
    "    all_seq_embed_local = embeddings[\"all_seq_embed_local\"]\n",
    "    all_seq_embed_by_position = embeddings[\"all_seq_embed_by_position\"]\n",
    "    all_seq_prob = embeddings[\"all_seq_prob\"]\n",
    "    \n",
    "    sequences_too_big_for_cuda = []\n",
    "\n",
    "    for i, seq in tqdm.tqdm(enumerate(all_sequences)):\n",
    "        # EXTRACT EMBEDDINGS, MUTATION PROBABILITIES, ENTROPY\n",
    "\n",
    "        # check the device is coherent with protein length\n",
    "        if (str(device) == \"cuda\" and len(seq) > MAX_CUDA_SEQ_LEN):\n",
    "            # if the protein is too big, don't try (we will do it with a cpu later)\n",
    "            sequences_too_big_for_cuda.append(seq)\n",
    "            continue\n",
    "        elif (str(device) == \"cpu\" and len(seq) <= MAX_CUDA_SEQ_LEN):\n",
    "            continue\n",
    "\n",
    "        data = [(\"_\", seq)]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = t_model(batch_tokens, repr_layers=[33])\n",
    "        # go from 33 to 20 (1 per amino acid)\n",
    "        logits = (results[\"logits\"].detach().cpu().numpy()[0, ].T)[4:24, 1:-1]\n",
    "        all_seq_prob[i] = softmax(logits, axis=0)\n",
    "        results = results[\"representations\"][33].detach().cpu().numpy()\n",
    "\n",
    "        # SAVE EMBEDDINGS\n",
    "        all_seq_embed_local[i] = results\n",
    "        all_seq_embed_pool[i, ] = np.mean(results[0, :, :], axis=0)\n",
    "\n",
    "        # TEMPORARILY SAVE LOCAL MUTATION EMBEDDINGS\n",
    "        mutation_positions = df.loc[df.sequence == seq,\n",
    "                                    \"mutation_position\"].unique().astype(int)\n",
    "\n",
    "        # the goal here is to fit the pca on the concat of all embeddings,\n",
    "        # therefore if one protein has 1000 single mutation it will appear 1000 times\n",
    "        # and we will overfit the pca to this protein\n",
    "        # => we choose max PCA_CT single mutations\n",
    "        if len(mutation_positions) > PCA_CT:\n",
    "            mutation_positions = np.random.choice(\n",
    "                mutation_positions, PCA_CT, replace=False)\n",
    "        for j in mutation_positions:\n",
    "            all_seq_embed_by_position[i] = results[0, j, :]\n",
    "\n",
    "        del batch_tokens, results\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    embeddings = {\n",
    "        \"all_seq_embed_pool\": all_seq_embed_pool,\n",
    "        \"all_seq_embed_local\": all_seq_embed_local,\n",
    "        \"all_seq_embed_by_position\": all_seq_embed_by_position,\n",
    "        \"all_seq_prob\": all_seq_prob,\n",
    "    }\n",
    "\n",
    "    return embeddings, sequences_too_big_for_cuda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Extracting embeddings from proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "471it [01:11,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0\n",
      "Extracting embeddings from proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "471it [00:00, 595753.07it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sequences = df.sequence.unique()\n",
    "embeddings = {\n",
    "    \"all_seq_embed_pool\": np.zeros((len(all_sequences), 1280)),\n",
    "    \"all_seq_embed_local\": [None]*len(all_sequences),\n",
    "    \"all_seq_embed_by_position\": [None]*len(all_sequences),\n",
    "    \"all_seq_prob\": [None]*len(all_sequences),\n",
    "}\n",
    "\n",
    "# first we do 'small' proteins with cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t_model.to(device)\n",
    "print(device)\n",
    "embeddings, sequences_too_big_for_cuda = extract_embeddings(\n",
    "    all_sequences, embeddings, t_model, device)\n",
    "\n",
    "# then we do the biggest proteins with cpu\n",
    "device = torch.device(\"cpu\")\n",
    "t_model.to(device)\n",
    "print(device)\n",
    "print(len(sequences_too_big_for_cuda))\n",
    "embeddings, sequences_too_big_for_cuda = extract_embeddings(\n",
    "    all_sequences, embeddings, t_model, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAPIDS PCA\n",
    "\n",
    "The transformer embeddings have dimension 1280. Since we only have a few thousand rows of train data, that is too many features to include all of them in our XGB model. Furthermore, we want to use local, pooling, and delta embeddings. Which would be 3x1280. To prevent our model from overfitting as a result of the \"curse of dimensionality\", we reduce the dimension of embeddings using RAPIDS PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sequence_to_embed_mapping\n",
    "sequence_to_embed_mapping = {seq: i for i, seq in enumerate(all_sequences)}\n",
    "# create stack\n",
    "all_seq_embed_by_position = np.stack(\n",
    "    embeddings.pop(\"all_seq_embed_by_position\"))\n",
    "pca_pool = PCA(n_components=32)\n",
    "pca_embeds = pca_pool.fit_transform(\n",
    "    embeddings.pop(\"all_seq_embed_pool\").astype(\"float32\"))\n",
    "pca_local = PCA(n_components=16)\n",
    "pca_local.fit(all_seq_embed_by_position.astype(\"float32\"))\n",
    "\n",
    "# we delete all_seq_embed_by_position: we only used it to fit the pca_local\n",
    "del all_seq_embed_by_position\n",
    "_ = gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(471, 32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(pca_embeds.shape)\n",
    "print(len(sequences_too_big_for_cuda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [f\"pca_pool_{k}\" for k in range(32)]\n",
    "new_columns += [f\"esm_pca_wild_{k}\" for k in range(16)]\n",
    "new_columns += [f\"esm_pca_mutant_{k}\" for k in range(16)]\n",
    "new_columns += [f\"esm_pca_local_{k}\" for k in range(16)]\n",
    "new_columns += [\"esm_mutation_probability\", \"esm_mutation_entropy\"]\n",
    "\n",
    "for col in new_columns:\n",
    "    df[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8734\n",
      "5389\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[~(df.ddG.isna())]\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embbeddings(row, sequence_to_embed_mapping, embeddings, t_model, device):\n",
    "    try:\n",
    "        ##################\n",
    "        # ROW - IS ROW FROM DOWNLOADED TRAIN CSV\n",
    "        ##################\n",
    "        # pdb_map = {x: y for x, y in zip(all_pdb, range(len(all_pdb)))}\n",
    "        atom_df = PandasPdb().read_pdb(row.alphafold_path)\n",
    "        atom_df = atom_df.df['ATOM']\n",
    "\n",
    "        residue_atoms = atom_df.loc[(\n",
    "            atom_df.residue_number == row.mutation_position)].reset_index(drop=True)\n",
    "\n",
    "        # FEATURE ENGINEER\n",
    "        if len(residue_atoms) > 0:\n",
    "\n",
    "            # check the device is coherent with protein length\n",
    "            if (str(device) == \"cuda\" and len(row.sequence) > MAX_CUDA_SEQ_LEN):\n",
    "                # if the protein is too big, don't try (we will do it with a cpu later)\n",
    "                return row\n",
    "            elif (str(device) == \"cpu\" and len(row.sequence) <= MAX_CUDA_SEQ_LEN):\n",
    "                return row\n",
    "                \n",
    "            # GET MUTANT EMBEDDINGS\n",
    "            mutated_sequence = (row.sequence[:row.mutation_position] +\n",
    "                                row.mutated_aa+row.sequence[row.mutation_position+1:])\n",
    "            data = [(\"_\", mutated_sequence)]\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            with torch.no_grad():\n",
    "                results = t_model(batch_tokens, repr_layers=[33])\n",
    "            results = results[\"representations\"][33].cpu().numpy()\n",
    "            mutant_local = pca_local.transform(\n",
    "                results[:1, row.mutation_position, :])[0, ]\n",
    "            mutant_pool = np.mean(results[:1, :, :], axis=1)\n",
    "            mutant_pool = pca_pool.transform(mutant_pool)[0, ]\n",
    "\n",
    "            # TRANSFORMER ESM EMBEDDINGS\n",
    "            wild_local = pca_local.transform(\n",
    "                embeddings[\"all_seq_embed_local\"][sequence_to_embed_mapping[row.sequence]][:1, row.mutation_position, :])[0, ]\n",
    "            wild_pool = pca_embeds[sequence_to_embed_mapping[row.sequence], ]\n",
    "            for k in range(32):\n",
    "                row[f\"esm_pca_pool_{k}\"] = mutant_pool[k] - wild_pool[k]\n",
    "            for k in range(16):\n",
    "                row[f\"esm_pca_wild_{k}\"] = wild_local[k]\n",
    "                row[f\"esm_pca_mutant_{k}\"] = mutant_local[k]\n",
    "                row[f\"esm_pca_local_{k}\"] = mutant_local[k] - wild_local[k]\n",
    "\n",
    "            # TRANSFORMER MUTATION PROBS AND ENTROPY\n",
    "            row[\"esm_mutation_probability\"] = embeddings[\"all_seq_prob\"][sequence_to_embed_mapping[row.sequence]\n",
    "                                        ][token_map[row.mutated_aa], row.mutation_position]\n",
    "            row[\"esm_mutation_entropy\"] = entropy(\n",
    "                embeddings[\"all_seq_prob\"][sequence_to_embed_mapping[row.sequence]][:, row.mutation_position])\n",
    "\n",
    "            del batch_tokens, results, mutant_local, mutant_pool, wild_local, wild_pool\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"error occured for {row.uniprot} {row.mutation_position} {row.mutated_aa}\")\n",
    "\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "df.mutation_position = df.mutation_position.astype(int)\n",
    "\n",
    "# first we do 'small' proteins with cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t_model.to(device)\n",
    "print(device)\n",
    "df = df.apply(lambda row: add_embbeddings(\n",
    "    row, sequence_to_embed_mapping, embeddings, t_model, device), axis=1)\n",
    "\n",
    "# then we do the biggest proteins with cpu\n",
    "device = torch.device(\"cpu\")\n",
    "t_model.to(device)\n",
    "print(device)\n",
    "print(len(sequences_too_big_for_cuda))\n",
    "df = df.apply(lambda row: add_embbeddings(\n",
    "    row, sequence_to_embed_mapping, embeddings, t_model, device), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot</th>\n",
       "      <th>wild_aa</th>\n",
       "      <th>mutated_chain</th>\n",
       "      <th>mutation_position</th>\n",
       "      <th>mutated_aa</th>\n",
       "      <th>pH</th>\n",
       "      <th>sequence</th>\n",
       "      <th>length</th>\n",
       "      <th>chain_start</th>\n",
       "      <th>chain_end</th>\n",
       "      <th>...</th>\n",
       "      <th>esm_pca_local_8</th>\n",
       "      <th>esm_pca_local_9</th>\n",
       "      <th>esm_pca_local_10</th>\n",
       "      <th>esm_pca_local_11</th>\n",
       "      <th>esm_pca_local_12</th>\n",
       "      <th>esm_pca_local_13</th>\n",
       "      <th>esm_pca_local_14</th>\n",
       "      <th>esm_pca_local_15</th>\n",
       "      <th>esm_mutation_probability</th>\n",
       "      <th>esm_mutation_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P09038</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>262</td>\n",
       "      <td>K</td>\n",
       "      <td>7.5</td>\n",
       "      <td>MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...</td>\n",
       "      <td>288.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P09038</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>262</td>\n",
       "      <td>C</td>\n",
       "      <td>7.5</td>\n",
       "      <td>MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...</td>\n",
       "      <td>288.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P09038</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>262</td>\n",
       "      <td>F</td>\n",
       "      <td>7.5</td>\n",
       "      <td>MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...</td>\n",
       "      <td>288.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P09038</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>262</td>\n",
       "      <td>A</td>\n",
       "      <td>7.5</td>\n",
       "      <td>MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...</td>\n",
       "      <td>288.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P09038</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>262</td>\n",
       "      <td>P</td>\n",
       "      <td>7.5</td>\n",
       "      <td>MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...</td>\n",
       "      <td>288.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot wild_aa mutated_chain  mutation_position mutated_aa   pH  \\\n",
       "0  P09038       T             A                262          K  7.5   \n",
       "1  P09038       T             A                262          C  7.5   \n",
       "2  P09038       T             A                262          F  7.5   \n",
       "3  P09038       T             A                262          A  7.5   \n",
       "4  P09038       T             A                262          P  7.5   \n",
       "\n",
       "                                            sequence  length  chain_start  \\\n",
       "0  MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...   288.0        142.0   \n",
       "1  MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...   288.0        142.0   \n",
       "2  MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...   288.0        142.0   \n",
       "3  MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...   288.0        142.0   \n",
       "4  MVGVGGGDVEDVTPRPGGCQISGRGARGCNGIPGAAAWEAALPRRR...   288.0        142.0   \n",
       "\n",
       "   chain_end  ... esm_pca_local_8  esm_pca_local_9  esm_pca_local_10  \\\n",
       "0      287.0  ...             NaN              NaN               NaN   \n",
       "1      287.0  ...             NaN              NaN               NaN   \n",
       "2      287.0  ...             NaN              NaN               NaN   \n",
       "3      287.0  ...             NaN              NaN               NaN   \n",
       "4      287.0  ...             NaN              NaN               NaN   \n",
       "\n",
       "   esm_pca_local_11 esm_pca_local_12  esm_pca_local_13 esm_pca_local_14  \\\n",
       "0               NaN              NaN               NaN              NaN   \n",
       "1               NaN              NaN               NaN              NaN   \n",
       "2               NaN              NaN               NaN              NaN   \n",
       "3               NaN              NaN               NaN              NaN   \n",
       "4               NaN              NaN               NaN              NaN   \n",
       "\n",
       "   esm_pca_local_15  esm_mutation_probability  esm_mutation_entropy  \n",
       "0               NaN                       NaN                   NaN  \n",
       "1               NaN                       NaN                   NaN  \n",
       "2               NaN                       NaN                   NaN  \n",
       "3               NaN                       NaN                   NaN  \n",
       "4               NaN                       NaN                   NaN  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_DATASET, index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
