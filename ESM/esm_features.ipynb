{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from cuml import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer ESM Features\n",
    "\n",
    "In order to convert amino acid sequences aka proteins into meaningful features, we will use embeddings from SOTA protein transformer. We use Facebook's pretrained protein transformer ESM (Evolutionary Scale Modeling) with research paper here and GitHub here. Kaggleqrdl provided a starter notebook here. In version 15+, we also extract mutation probabilties and mutation entropy from ESM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"all_v2_2\"\n",
    "INPUT_DATASET = f\"../data/main_dataset_creation/outputs/{DATASET_NAME}/dataset_with_alphafold_paths.csv\"\n",
    "OUTPUT_DATASET = f\"../data/main_dataset_creation/outputs/{DATASET_NAME}/dataset_with_esm_features.csv\"\n",
    "MAX_CUDA_SEQ_LEN = 500 # out of memory w/ the 3070 after this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/kaggleqrdl/esm-quick-start-lb237\n",
    "\n",
    "token_map = {'L': 0, 'A': 1, 'G': 2, 'V': 3, 'S': 4, 'E': 5, 'R': 6, 'T': 7, 'I': 8, 'D': 9, 'P': 10,\n",
    "             'K': 11, 'Q': 12, 'N': 13, 'F': 14, 'Y': 15, 'M': 16, 'H': 17, 'W': 18, 'C': 19}\n",
    "t_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "t_model.eval()  # disables dropout for deterministic results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "t_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "We input each train and test wildtype into our transformer and extract the last hidden layers activations. For each protein, this has shape (1, len_protein_seq, 1280). We will save the full embeddings and the pooled embeddings for use later. Additionally we will save the MLM pretrain task amino acid prediction which indicates mutation probability and mutation entropy. This has shape (1, len_protein_seq, 33) but we extract to (len_protein_seq, 20) where 20 is number of common amino acids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_DATASET)\n",
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "477it [00:44, 10.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST WILDTYPES\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "PCA_CT = 16  # random sample size per protein to fit PCA with\n",
    "all_sequences = df.sequence.unique()\n",
    "all_seq_embed_pool = np.zeros((len(all_sequences)+1, 1280))\n",
    "all_seq_embed_local = []\n",
    "all_seq_embed_by_position = []\n",
    "all_seq_prob = []\n",
    "seq_to_embed = {} # {\"MV...\": (i, [list of all positions])}\n",
    "seq_to_big_for_cuda = []\n",
    "\n",
    "# EXTRACT TRANSFORMER EMBEDDINGS FOR TRAIN AND TEST WILDTYPES\n",
    "print('Extracting embeddings from proteins...')\n",
    "for i, seq in tqdm.tqdm(enumerate(all_sequences)):\n",
    "    # EXTRACT EMBEDDINGS, MUTATION PROBABILITIES, ENTROPY\n",
    "    \n",
    "    if len(seq)>MAX_CUDA_SEQ_LEN:\n",
    "        # if the protein is too big, don't try (we will do it with a cpu later)\n",
    "        seq_to_big_for_cuda.append(seq)\n",
    "        continue\n",
    "\n",
    "\n",
    "    data = [(\"protein1\", seq)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "        results = t_model(batch_tokens, repr_layers=[33])\n",
    "    logits = (results['logits'].detach().cpu().numpy()[0, ].T)[4:24, 1:-1]\n",
    "    all_seq_prob.append(softmax(logits, axis=0))\n",
    "    results = results[\"representations\"][33].detach().cpu().numpy()\n",
    "\n",
    "    # SAVE EMBEDDINGS\n",
    "    all_seq_embed_local.append(results)\n",
    "    all_seq_embed_pool[i, ] = np.mean(results[0, :, :], axis=0)\n",
    "\n",
    "    # TEMPORARILY SAVE LOCAL MUTATION EMBEDDINGS\n",
    "    mutation_positions = df.loc[df.sequence == seq,\n",
    "                                'mutation_position'].unique().astype(int)\n",
    "    # update seq_to_embed mapping\n",
    "    seq_to_embed[seq] = (i, mutation_positions)\n",
    "    # if len(tmp) > PCA_CT:\n",
    "    #     tmp = np.random.choice(tmp, PCA_CT, replace=False)\n",
    "    for j in mutation_positions:\n",
    "        all_seq_embed_by_position.append(results[0, j, :])\n",
    "\n",
    "    del batch_tokens, results\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_seq_embed_by_position = np.stack(all_seq_embed_by_position)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAPIDS PCA\n",
    "\n",
    "The transformer embeddings have dimension 1280. Since we only have a few thousand rows of train data, that is too many features to include all of them in our XGB model. Furthermore, we want to use local, pooling, and delta embeddings. Which would be 3x1280. To prevent our model from overfitting as a result of the \"curse of dimensionality\", we reduce the dimension of embeddings using RAPIDS PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_pool = PCA(n_components=32)\n",
    "pca_embeds = pca_pool.fit_transform(all_seq_embed_pool.astype('float32'))\n",
    "pca_local = PCA(n_components=16)\n",
    "pca_local.fit(all_seq_embed_by_position.astype('float32'))\n",
    "# del all_seq_embed_by_position\n",
    "# _ = gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_embeds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_row(atom_df, j, row):\n",
    "    ##################\n",
    "    # ATOM_DF - IS PDB FILE'S ATOM_DF\n",
    "    # J - IS RESIDUE NUMBER WHICH IS TRAIN CSV POSITION PLUS OFFSET\n",
    "    # ROW - IS ROW FROM DOWNLOADED TRAIN CSV\n",
    "    ##################\n",
    "\n",
    "    dd = None\n",
    "    tmp = atom_df.loc[(atom_df.residue_number == j)].reset_index(drop=True)\n",
    "    prev = atom_df.loc[(atom_df.residue_number == j-1)].reset_index(drop=True)\n",
    "    post = atom_df.loc[(atom_df.residue_number == j+1)].reset_index(drop=True)\n",
    "\n",
    "    # FEATURE ENGINEER\n",
    "    if len(tmp) > 0:\n",
    "\n",
    "        # GET MUTANT EMBEDDINGS\n",
    "        data = [(\"protein1\", row.mutant_seq)]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = t_model(batch_tokens, repr_layers=[33])\n",
    "        results = results[\"representations\"][33].cpu().numpy()\n",
    "        mutant_local = pca_local.transform(results[:1, row.position, :])[0, ]\n",
    "        mutant_pool = np.mean(results[:1, :, :], axis=1)\n",
    "        mutant_pool = pca_pool.transform(mutant_pool)[0, ]\n",
    "\n",
    "        # MUTATION AND POSITION\n",
    "        dd = {}\n",
    "        dd['WT'] = row.wildtype\n",
    "        dd['WT2'] = tmp.residue_name.map(aa_map)[0]\n",
    "        dd['MUT'] = row.mutation\n",
    "        dd['position'] = row.position\n",
    "        dd['relative_position'] = row.position / len(row.sequence)\n",
    "\n",
    "        # B_FACTOR\n",
    "        if USE_B_COLUMN:\n",
    "            dd['b_factor'] = tmp.b_factor.mean()\n",
    "\n",
    "        # ANIMO ACID PROPERTIES AND DELTAS\n",
    "        for c in PROPS:\n",
    "            dd[f'{c}_1'] = aa_props.loc[row.wildtype, c]\n",
    "            dd[f'{c}_2'] = aa_props.loc[row.mutation, c]\n",
    "            dd[f'{c}_delta'] = dd[f'{c}_2']-dd[f'{c}_1']\n",
    "\n",
    "        # SUBSTITUTION MATRICES\n",
    "        dd['blosum100'] = sub_mat_b100[(row.wildtype, row.mutation)]\n",
    "        dd['blosum80'] = sub_mat_b80[(row.wildtype, row.mutation)]\n",
    "        dd['blosum60'] = sub_mat_b60[(row.wildtype, row.mutation)]\n",
    "        dd['blosum40'] = sub_mat_b40[(row.wildtype, row.mutation)]\n",
    "        dd['demask'] = sub_mat_demask[(row.wildtype, row.mutation)]\n",
    "\n",
    "        # PREVIOUS AND POST AMINO ACID INFO\n",
    "        if (len(prev) > 0):\n",
    "            dd['prev'] = prev.residue_name.map(aa_map)[0]\n",
    "            if USE_B_COLUMN:\n",
    "                dd['b_factor_prev'] = prev.b_factor.mean()\n",
    "        else:\n",
    "            dd['prev'] = 'X'\n",
    "            if USE_B_COLUMN:\n",
    "                dd['b_factor_prev'] = -999\n",
    "\n",
    "        if (len(post) > 0):\n",
    "            dd['post'] = post.residue_name.map(aa_map)[0]\n",
    "            if USE_B_COLUMN:\n",
    "                dd['b_factor_post'] = post.b_factor.mean()\n",
    "        else:\n",
    "            dd['post'] = 'X'\n",
    "            if USE_B_COLUMN:\n",
    "                dd['b_factor_post'] = -999\n",
    "\n",
    "        # ANGLE BETWEEN MUTATION AND NEIGHBORS\n",
    "        if (len(prev) > 0) & (len(post) > 0):\n",
    "            # BACKBONE ATOMS\n",
    "            atm = ['N', 'H', 'CA', 'O']\n",
    "            prev = prev.loc[prev.atom_name.isin(atm)]\n",
    "            tmp = tmp.loc[tmp.atom_name.isin(atm)]\n",
    "            post = post.loc[post.atom_name.isin(atm)]\n",
    "            # VECTORS\n",
    "            c_prev = np.array(\n",
    "                [prev.x_coord.mean(), prev.y_coord.mean(), prev.z_coord.mean()])\n",
    "            c_tmp = np.array(\n",
    "                [tmp.x_coord.mean(), tmp.y_coord.mean(), tmp.z_coord.mean()])\n",
    "            c_post = np.array(\n",
    "                [post.x_coord.mean(), post.y_coord.mean(), post.z_coord.mean()])\n",
    "            vec_a = c_prev - c_tmp\n",
    "            vec_b = c_post - c_tmp\n",
    "            # COMPUTE ANGLE\n",
    "            norm_a = np.sqrt(vec_a.dot(vec_a))\n",
    "            norm_b = np.sqrt(vec_b.dot(vec_b))\n",
    "            dd['cos_angle'] = vec_a.dot(vec_b)/norm_a/norm_b\n",
    "        else:\n",
    "            dd['cos_angle'] = -2\n",
    "\n",
    "        # 3D LOCATION OF MUTATION\n",
    "        atm = ['N', 'H', 'CA', 'O']\n",
    "        atoms = atom_df.loc[atom_df.atom_name.isin(atm)]\n",
    "        centroid1 = np.array(\n",
    "            [atoms.x_coord.mean(), atoms.y_coord.mean(), atoms.z_coord.mean()])\n",
    "        tmp = tmp.loc[tmp.atom_name.isin(atm)]\n",
    "        centroid2 = np.array(\n",
    "            [tmp.x_coord.mean(), tmp.y_coord.mean(), tmp.z_coord.mean()])\n",
    "        dist = centroid2 - centroid1\n",
    "        dd['location3d'] = dist.dot(dist)\n",
    "\n",
    "        # TRANSFORMER ESM EMBEDDINGS\n",
    "        wt_local = pca_local.transform(\n",
    "            all_pdb_embed_local[pdb_map[row.PDB]][:1, row.position, :])[0, ]\n",
    "        wt_pool = pca_embeds[pdb_map[row.PDB], ]\n",
    "        for kk in range(32):\n",
    "            dd[f'pca_pool_{kk}'] = mutant_pool[kk] - wt_pool[kk]\n",
    "            if kk >= 16:\n",
    "                continue\n",
    "            dd[f'pca_wt_{kk}'] = wt_local[kk]\n",
    "            dd[f'pca_mutant_{kk}'] = mutant_local[kk]\n",
    "            dd[f'pca_local_{kk}'] = mutant_local[kk] - wt_local[kk]\n",
    "\n",
    "        # TRANSFORMER MUTATION PROBS AND ENTROPY\n",
    "        dd['mut_prob'] = all_pdb_prob[pdb_map[row.PDB]\n",
    "                                      ][token_map[dd['MUT']], dd['position']-1]\n",
    "        dd['mut_entropy'] = entropy(\n",
    "            all_pdb_prob[pdb_map[row.PDB]][:, dd['position']-1])\n",
    "\n",
    "        # SURFACE AREA FEATURES\n",
    "        PATH = '../input/nesp-kaggle-train-surface-area/'\n",
    "        if row.CIF:\n",
    "            nm = f'{row.CIF}-model_v3.csv'\n",
    "        elif row.PDB != 'kaggle':\n",
    "            PATH = '../input/nesp-jin-external-surface-area/'\n",
    "            nm = f'{row.PDB}.csv'\n",
    "        else:\n",
    "            nm = 'wildtype_structure_prediction_af2_SASA.csv'\n",
    "        try:\n",
    "            area = pd.read_csv(f'{PATH}{nm}')\n",
    "            rw = area.loc[area.Residue_number == j].iloc[0]\n",
    "            dd['sa_total'] = rw.Total\n",
    "            dd['sa_apolar'] = rw.Apolar\n",
    "            dd['sa_backbone'] = rw.Backbone\n",
    "            dd['sa_sidechain'] = rw.Sidechain\n",
    "            dd['sa_ratio'] = rw.Ratio\n",
    "            dd['sa_in/out'] = -1\n",
    "            if rw['In/Out'] == 'i':\n",
    "                dd['sa_in/out'] = 1\n",
    "            elif rw['In/Out'] == 'o':\n",
    "                dd['sa_in/out'] = 0\n",
    "        except:\n",
    "            print('### NEED SURFACE AREA for PDB:',\n",
    "                  row.PDB, 'residue_number:', j)\n",
    "            return None\n",
    "\n",
    "        # LABEL ENCODE AMINO ACIDS\n",
    "        dd['AA1'] = aa_map_2[dd['WT']]\n",
    "        dd['AA2'] = aa_map_2[dd['MUT']]\n",
    "        dd['AA3'] = aa_map_2[dd['prev']]\n",
    "        dd['AA4'] = aa_map_2[dd['post']]\n",
    "\n",
    "        # TARGETS AND SOURCES\n",
    "        dd['ddG'] = row.ddG\n",
    "        dd['dTm'] = row.dTm\n",
    "        dd['pdb'] = row.PDB\n",
    "        dd['source'] = row.source\n",
    "\n",
    "        del batch_tokens, results, mutant_local, mutant_pool, wt_local, wt_pool\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return dd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
