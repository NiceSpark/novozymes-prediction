{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from cuml import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"all_v2_2\"\n",
    "INPUT_DATASET = f\"../data/main_dataset_creation/outputs/{DATASET_NAME}/dataset_with_alphafold_paths.csv\"\n",
    "OUTPUT_DATASET = f\"../data/main_dataset_creation/outputs/{DATASET_NAME}/dataset_with_esm_features.csv\"\n",
    "MAX_CUDA_SEQ_LEN = 6000 # out of memory w/ the 3070 after this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/kaggleqrdl/esm-quick-start-lb237\n",
    "\n",
    "token_map = {'L': 0, 'A': 1, 'G': 2, 'V': 3, 'S': 4, 'E': 5, 'R': 6, 'T': 7, 'I': 8, 'D': 9, 'P': 10,\n",
    "             'K': 11, 'Q': 12, 'N': 13, 'F': 14, 'Y': 15, 'M': 16, 'H': 17, 'W': 18, 'C': 19}\n",
    "t_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "t_model.eval()  # disables dropout for deterministic results\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "t_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "We input each train and test wildtype into our transformer and extract the last hidden layers activations. For each protein, this has shape (1, len_protein_seq, 1280). We will save the full embeddings and the pooled embeddings for use later. Additionally we will save the MLM pretrain task amino acid prediction which indicates mutation probability and mutation entropy. This has shape (1, len_protein_seq, 33) but we extract to (len_protein_seq, 20) where 20 is number of common amino acids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniprot',\n",
       " 'wild_aa',\n",
       " 'mutated_chain',\n",
       " 'mutation_position',\n",
       " 'mutated_aa',\n",
       " 'pH',\n",
       " 'sequence',\n",
       " 'length',\n",
       " 'chain_start',\n",
       " 'chain_end',\n",
       " 'AlphaFoldDB',\n",
       " 'Tm',\n",
       " 'ddG',\n",
       " 'dTm',\n",
       " 'dataset_source',\n",
       " 'infos_found',\n",
       " 'alphafold_path']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_DATASET)\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from proteins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "477it [01:17,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST WILDTYPES\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "PCA_CT = 16  # random sample size per protein to fit PCA with\n",
    "all_sequences = df.sequence.unique()\n",
    "all_pdb_embed_pool = np.zeros((len(all_sequences)+1, 1280))\n",
    "all_pdb_embed_local = []\n",
    "all_pdb_embed_tmp = []\n",
    "\n",
    "all_pdb_prob = []\n",
    "\n",
    "# EXTRACT TRANSFORMER EMBEDDINGS FOR TRAIN AND TEST WILDTYPES\n",
    "print('Extracting embeddings from proteins...')\n",
    "for i, seq in tqdm.tqdm(enumerate(all_sequences)):\n",
    "    # EXTRACT EMBEDDINGS, MUTATION PROBABILITIES, ENTROPY\n",
    "    if len(seq)>MAX_CUDA_SEQ_LEN:\n",
    "        continue\n",
    "    data = [(\"protein1\", seq)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "        results = t_model(batch_tokens, repr_layers=[33])\n",
    "    logits = (results['logits'].detach().cpu().numpy()[0, ].T)[4:24, 1:-1]\n",
    "    all_pdb_prob.append(softmax(logits, axis=0))\n",
    "    results = results[\"representations\"][33].detach().cpu().numpy()\n",
    "\n",
    "    # SAVE EMBEDDINGS\n",
    "    all_pdb_embed_local.append(results)\n",
    "    all_pdb_embed_pool[i, ] = np.mean(results[0, :, :], axis=0)\n",
    "\n",
    "    # TEMPORARILY SAVE LOCAL MUTATION EMBEDDINGS\n",
    "    tmp = df.loc[df.sequence == seq, 'mutation_position'].unique()\n",
    "    # if len(tmp) > PCA_CT:\n",
    "    #     tmp = np.random.choice(tmp, PCA_CT, replace=False)\n",
    "    for j in tmp:\n",
    "        all_pdb_embed_tmp.append(results[0, int(j), :])\n",
    "\n",
    "    del batch_tokens, results\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_pdb_embed_tmp = np.stack(all_pdb_embed_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
