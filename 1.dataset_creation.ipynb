{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "\n",
    "![flowchart](./doc/dataset_creation_flowchart.drawio.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.file_utils import open_json, write_json\n",
    "from utils.dataset_creation import *\n",
    "from utils.dataset_mapping import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "UPDATE_MAPPING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"pdbs\", \"uniprot\", \"wild_aa\", \"mutated_chain\", \"mutation_position\",\n",
    "           \"mutated_aa\", \"pH\",\n",
    "           \"sequence\", \"length\", \"chain_start\", \"chain_end\",\n",
    "           \"AlphaFoldDB\", \"Tm\", \"ddG\", \"dTm\",\n",
    "           \"dataset_source\", \"source_id\", \"infos_found\"]\n",
    "\n",
    "SUBSET_DUPLICATES = [\"wild_aa\", \"mutation_position\",\n",
    "                     \"mutated_aa\", \"pH\", \"sequence\"]\n",
    "\n",
    "NAME = \"all_dTm_source_id\"\n",
    "DIR = \"./data/main_dataset_creation\"\n",
    "OUTPUT_DIR = DIR+'/outputs/'+NAME\n",
    "\n",
    "LOCAL_UNIPROT_INFOS_PATH = DIR+\"/uniprot_infos.json\"\n",
    "PDB_UNIPROT_MAPPING_PATH = DIR+\"/mapping/pdb_uniprot_mapping.json\"\n",
    "LINKED_UNIPROT_MAPPING_PATH = DIR+\"/mapping/linked_uniprot_mapping.json\"\n",
    "SEQUENCE_UNIPROT_MAPPING_PATH = DIR + \\\n",
    "    \"/mapping/sequence_uniprot_mapping.json\"\n",
    "PDB_NO_UNIPROT_PATH = DIR+\"/mapping/pdb_no_uniprot.json\"\n",
    "SEQUENCE_NO_UNIPROT_PATH = DIR+\"/mapping/sequence_no_uniprot.json\"\n",
    "\n",
    "DATASET_OUTPUT_PATH_RAW = OUTPUT_DIR+f\"/dataset_raw.csv\"\n",
    "DATASET_OUTPUT_PATH_ONLY_INFOS = OUTPUT_DIR+f\"/dataset_only_infos.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infos for dataset creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 578 uniprot infos from local storage\n"
     ]
    }
   ],
   "source": [
    "local_uniprot_infos = open_json(LOCAL_UNIPROT_INFOS_PATH)\n",
    "dataset_config = open_json(DIR+\"/dataset_config.json\")\n",
    "\n",
    "print(f\"loaded {len(local_uniprot_infos)} uniprot infos from local storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating ./data/main_dataset_creation/outputs/all_dTm_source_id folder\n"
     ]
    }
   ],
   "source": [
    "# prepare output dir\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    print(f\"creating {OUTPUT_DIR} folder\")\n",
    "    os.mkdir(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all the required dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22565/3735331379.py:30: DtypeWarning: Columns (23,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(individual_config[\"data_path\"],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target is dTm, so we removed 12098 rows\n",
      "processed fireprotdb:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 226, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 6118 rows\n",
      "processed thermomutdb:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 2348, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 2557 rows\n",
      "processed O2567_new:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 3639 rows\n",
      "processed prothermdb:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 1112, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 630 rows\n",
      "processed S630:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 3568 rows\n",
      "processed S3568:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 168 rows\n",
      "processed jinyuan_sun_test:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 4048 rows\n",
      "processed jinyuan_sun_train:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 5444 rows\n",
      "processed datasetDDG_train:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 1382, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 276 rows\n",
      "processed datasetDDG_test:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 73, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 4098 rows\n",
      "processed all_train_data_v17:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 140 rows\n",
      "processed S140:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 2648 rows\n",
      "processed S2648:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 1744 rows\n",
      "processed Q1744:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 3214 rows\n",
      "processed Q3214:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n",
      "target is dTm, so we removed 3421 rows\n",
      "processed Q3421:\n",
      "errors={'no_sequence_in_data': 0, 'not_in_local': 0, 'wrong_position': 0, 'no_uniprot': 0, 'no_pdb': 0, 'no_sequence': 0}\n"
     ]
    }
   ],
   "source": [
    "if not FRESH_START:\n",
    "    main_df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "else:\n",
    "    main_df = pd.DataFrame()\n",
    "    main_df = add_missing_column(main_df, COLUMNS)\n",
    "\n",
    "    for dataset_source in dataset_config[\"dataset_to_process\"]:\n",
    "        errors = {\n",
    "            \"no_sequence_in_data\": 0,\n",
    "            \"not_in_local\": 0,\n",
    "            \"wrong_position\": 0,\n",
    "            \"no_uniprot\": 0,\n",
    "            \"no_pdb\": 0,\n",
    "            \"no_sequence\": 0,\n",
    "        }\n",
    "\n",
    "        individual_config = dataset_config[dataset_source]\n",
    "        # load csv\n",
    "        if dataset_source == \"thermomutdb\":\n",
    "            df = pd.read_json(individual_config[\"data_path\"])\n",
    "            df = df[df.mut_count.eq(0)]\n",
    "            df[df.uniprot.eq('-')] = np.nan\n",
    "        elif dataset_source == \"Q3421\":\n",
    "            df = pd.read_csv(individual_config[\"data_path\"],\n",
    "                             delimiter=\"\\s+\").iloc[1:].reset_index(drop=True)\n",
    "            df.ddG = df.ddG.astype(float)\n",
    "            df.pH = df.pH.astype(float)\n",
    "            df[\"Pos(PDB)\"] = df[\"Pos(PDB)\"].astype(float)\n",
    "        else:\n",
    "            df = pd.read_csv(individual_config[\"data_path\"],\n",
    "                             sep=individual_config.get(\"sep\", ',')).drop_duplicates()\n",
    "        \n",
    "        if dataset_source == \"fireprotdb\":\n",
    "            # Invalid PDB structures, we use AF2 structures in FireProtDB\n",
    "            df = df.dropna(subset=['pdb_id']).reset_index(drop=True)\n",
    "\n",
    "        # rename columns\n",
    "        df.rename(columns=individual_config[\"renaming_dict\"],\n",
    "                  inplace=True)\n",
    "        if dataset_source in [\"S140\", \"S2648\", \"Q1744\", \"Q3214\"]:\n",
    "            df[\"mutated_chain\"] = df.pdbs.str[-1]\n",
    "            df.pdbs = df.pdbs.str[:-1]\n",
    "        # add missing columns\n",
    "        df = add_missing_column(df, COLUMNS)\n",
    "        # ad source_id\n",
    "        df[\"source_id\"] = dataset_source+\"_\"+df.index.astype(str)\n",
    "\n",
    "        # split mutation code if needed\n",
    "        if individual_config[\"need_mutation_code_split\"]:\n",
    "            df = df.apply(apply_split_mutation_code, axis=1)\n",
    "        # remove nan mutation_code\n",
    "        df = df[~df[\"mutation_position\"].isna()]\n",
    "        # keep only COLUMNS\n",
    "        df = df[COLUMNS]\n",
    "        # drop duplicates\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        # add dataset_source\n",
    "        df[\"dataset_source\"] = dataset_source\n",
    "        # index start at 0\n",
    "        df[\"mutation_position\"] = df[\"mutation_position\"].apply(lambda x: x-1)\n",
    "        # max precision of pH: .1 (to avoid duplicates)\n",
    "        df[\"pH\"] = df[\"pH\"].round(1)\n",
    "\n",
    "        # apply target corrections\n",
    "        df[\"ddG\"] = df[\"ddG\"].apply(\n",
    "            lambda x: x*individual_config[\"corrections\"][\"ddG\"])\n",
    "        df[\"dTm\"] = df[\"dTm\"].apply(\n",
    "            lambda x: x*individual_config[\"corrections\"][\"dTm\"])\n",
    "        # better to initialize infos_found at 0 than nan\n",
    "        df[\"infos_found\"] = 0\n",
    "        # by default chain is \"A\"\n",
    "        df[\"mutated_chain\"].fillna(\"A\")\n",
    "        df[\"mutated_chain\"] = df[\"mutated_chain\"].astype(\"str\")\n",
    "        df[\"mutated_chain\"].str.replace('0', 'A')\n",
    "\n",
    "        # check number of rows without uniprot\n",
    "        # check validity of uniprot, and add the infos for those\n",
    "        df = df.apply(lambda row: apply_valid_uniprot(\n",
    "            row, local_uniprot_infos, dataset_config, errors), axis=1)\n",
    "        \n",
    "        # if target is not \"all\", we keep only the target, \n",
    "        # in order to make sure data is not lost when we remove duplicates\n",
    "        if dataset_config[\"general_config\"][\"target\"]==\"ddG\":\n",
    "            l = len(df)\n",
    "            df = df[~(df.ddG.isna())]\n",
    "            print(f\"target is ddG, so we removed {l-len(df)} rows\")\n",
    "        elif dataset_config[\"general_config\"][\"target\"] == \"dTm\":\n",
    "            l = len(df)\n",
    "            df = df[~(df.dTm.isna())]\n",
    "            print(f\"target is dTm, so we removed {l-len(df)} rows\")\n",
    "\n",
    "\n",
    "        print(f\"processed {dataset_source}:\")\n",
    "        print(f\"{errors=}\")\n",
    "\n",
    "        main_df = pd.concat([main_df, df], ignore_index=True)\n",
    "        main_df.drop_duplicates(SUBSET_DUPLICATES, inplace=True)\n",
    "\n",
    "    # save\n",
    "    write_json(LOCAL_UNIPROT_INFOS_PATH, local_uniprot_infos)\n",
    "    main_df.to_csv(DATASET_OUTPUT_PATH_RAW, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update mapping and try to add infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not UPDATE_MAPPING:\n",
    "    # don't go beyond here with Run All\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 0 entries to pdb_uniprot_mapping\n"
     ]
    }
   ],
   "source": [
    "# update pdb to uniprot mapping\n",
    "update_pdb_uniprot_mapping(LOCAL_UNIPROT_INFOS_PATH,\n",
    "                           PDB_UNIPROT_MAPPING_PATH,\n",
    "                           LINKED_UNIPROT_MAPPING_PATH)\n",
    "\n",
    "pdb_uniprot_mapping = open_json(PDB_UNIPROT_MAPPING_PATH)\n",
    "linked_uniprot_mapping = open_json(LINKED_UNIPROT_MAPPING_PATH)\n",
    "pdb_without_uniprot = open_json(PDB_NO_UNIPROT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 546.0 new infos thanks to uniprot_from_pdb\n"
     ]
    }
   ],
   "source": [
    "# add infos based on pdb not uniprot\n",
    "df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "\n",
    "with_infos = df.infos_found.sum()\n",
    "df = df.apply(lambda row: apply_infos_from_pdb(row, local_uniprot_infos, pdb_uniprot_mapping,\n",
    "                                               linked_uniprot_mapping, dataset_config,\n",
    "                                               pdb_without_uniprot, errors),\n",
    "              axis=1)\n",
    "print(\n",
    "    f\"added {df.infos_found.sum()-with_infos} new infos thanks to uniprot_from_pdb\")\n",
    "df.to_csv(DATASET_OUTPUT_PATH_RAW, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 0 entries to sequence_uniprot_mapping\n"
     ]
    }
   ],
   "source": [
    "# update sequence to uniprot mapping\n",
    "update_sequence_uniprot_mapping(LOCAL_UNIPROT_INFOS_PATH,\n",
    "                                SEQUENCE_UNIPROT_MAPPING_PATH,\n",
    "                                LINKED_UNIPROT_MAPPING_PATH)\n",
    "\n",
    "sequence_uniprot_mapping = open_json(SEQUENCE_UNIPROT_MAPPING_PATH)\n",
    "sequence_without_uniprot = open_json(SEQUENCE_NO_UNIPROT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 916.0 new infos thanks to uniprot_from_sequence\n"
     ]
    }
   ],
   "source": [
    "# add infos based on sequence not pdb or uniprot\n",
    "\n",
    "df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "\n",
    "with_infos = df.infos_found.sum()\n",
    "df = df.apply(lambda row: apply_infos_from_sequence(row, local_uniprot_infos, sequence_uniprot_mapping,\n",
    "                                                    linked_uniprot_mapping, dataset_config,\n",
    "                                                    sequence_without_uniprot, errors),\n",
    "              axis=1)\n",
    "print(\n",
    "    f\"added {df.infos_found.sum()-with_infos} new infos thanks to uniprot_from_sequence\")\n",
    "\n",
    "df.to_csv(DATASET_OUTPUT_PATH_RAW, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure mapping and other data is saved\n",
    "write_json(LOCAL_UNIPROT_INFOS_PATH, local_uniprot_infos)\n",
    "write_json(PDB_NO_UNIPROT_PATH, pdb_without_uniprot)\n",
    "write_json(SEQUENCE_NO_UNIPROT_PATH, sequence_without_uniprot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_chain(row):\n",
    "    if type(row[\"mutated_chain\"]) != type(\"\"):\n",
    "        row[\"mutated_chain\"] = \"A\"\n",
    "    if len(row[\"mutated_chain\"]) != 1:\n",
    "        row[\"mutated_chain\"] = \"A\"\n",
    "    if row[\"mutated_chain\"] in ['_', '-']:\n",
    "        row[\"mutated_chain\"] = \"A\"\n",
    "\n",
    "    row[\"mutated_chain\"] = row[\"mutated_chain\"].upper()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1255\n",
      "1 8102\n",
      "2 6847\n",
      "num_no_pH=1549\n",
      "3 5298\n",
      "4 5298\n",
      "5 5146\n"
     ]
    }
   ],
   "source": [
    "SEARCH_ON = ['uniprot', 'wild_aa', 'mutation_position',\n",
    "             'mutated_aa', 'sequence']  # same as SUBSET but without pH\n",
    "\n",
    "main_df = pd.read_csv(DATASET_OUTPUT_PATH_RAW)\n",
    "dataset_config = open_json(DIR+\"/dataset_config.json\")\n",
    "# remove pdbs\n",
    "main_df = main_df[COLUMNS[1:]]\n",
    "print(0, main_df.infos_found.eq(0.0).sum())\n",
    "\n",
    "# remove record without uniprot infos\n",
    "print(1, len(main_df))\n",
    "main_df = main_df.loc[main_df.infos_found == 1]\n",
    "print(2, len(main_df))\n",
    "mean_pH = main_df.pH.mean()\n",
    "count = 0\n",
    "num_no_pH = len(main_df.loc[main_df.pH.isna()])\n",
    "print(f\"{num_no_pH=}\")\n",
    "\n",
    "# # try to find if rows without pH are already somewhere else in the df\n",
    "# for _, row in main_df.loc[main_df.pH.isna()].iterrows():\n",
    "#     if (len(main_df.loc[(main_df[\"wild_aa\"] == row[\"wild_aa\"]) &\n",
    "#                         (main_df[\"mutation_position\"] == row[\"mutation_position\"]) &\n",
    "#                         (main_df[\"mutated_aa\"] == row[\"mutated_aa\"]) &\n",
    "#                         (main_df[\"sequence\"] == row[\"sequence\"])]) == 0):\n",
    "#         # the row with no pH is not present in the rest of the df\n",
    "#         row[\"pH\"] = mean_pH\n",
    "#         count += 1\n",
    "# print(f\"out of {num_no_pH} rows without pH we kept {count} because those had no duplicate\")\n",
    "# #  ==> always 0\n",
    "\n",
    "\n",
    "main_df = main_df.loc[~main_df.pH.isna()]\n",
    "print(3, len(main_df))\n",
    "\n",
    "# check for errors in chain and correct them\n",
    "main_df = main_df.apply(correct_chain, axis=1)\n",
    "# remove duplicates\n",
    "print(4, len(main_df))\n",
    "main_df.drop_duplicates(subset=SUBSET_DUPLICATES, inplace=True)\n",
    "print(5, len(main_df))\n",
    "\n",
    "dataset_infos = {\n",
    "    \"total_len\": len(main_df),\n",
    "    \"dataset_processed\": dataset_config[\"dataset_to_process\"],\n",
    "    \"general_config\": dataset_config[\"general_config\"],\n",
    "    \"dataset_source_repartition\": main_df.dataset_source.value_counts().to_dict(),\n",
    "    \"unique_uniprot\": len(main_df.uniprot.unique()),\n",
    "    \"ddG\": (len(main_df)-main_df.ddG.isna().sum()),\n",
    "    \"dTm\": (len(main_df)-main_df.dTm.isna().sum()),\n",
    "    \"Tm\": (len(main_df)-main_df.Tm.isna().sum()),\n",
    "    \"nan_repartition\": main_df.isna().sum().to_dict(),\n",
    "    \"no_pH_repartition\": main_df[main_df.pH.isna()].dataset_source.value_counts().to_dict(),\n",
    "}\n",
    "\n",
    "main_df.to_csv(DATASET_OUTPUT_PATH_ONLY_INFOS, index=False)\n",
    "write_json(OUTPUT_DIR+\"/dataset_config.json\", dataset_config)\n",
    "write_json(OUTPUT_DIR+\"/dataset_infos.json\", dataset_infos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tm': 3019,\n",
      " 'dTm': 5146,\n",
      " 'dataset_processed': ['fireprotdb',\n",
      "                       'thermomutdb',\n",
      "                       'O2567_new',\n",
      "                       'prothermdb',\n",
      "                       'S630',\n",
      "                       'S3568',\n",
      "                       'jinyuan_sun_test',\n",
      "                       'jinyuan_sun_train',\n",
      "                       'datasetDDG_train',\n",
      "                       'datasetDDG_test',\n",
      "                       'all_train_data_v17',\n",
      "                       'S140',\n",
      "                       'S2648',\n",
      "                       'Q1744',\n",
      "                       'Q3214',\n",
      "                       'Q3421'],\n",
      " 'dataset_source_repartition': {'fireprotdb': 2735,\n",
      "                                'prothermdb': 385,\n",
      "                                'thermomutdb': 2026},\n",
      " 'ddG': 593,\n",
      " 'general_config': {'fill_na_pH': False, 'target': 'dTm'},\n",
      " 'nan_repartition': {'AlphaFoldDB': 1049,\n",
      "                     'Tm': 2127,\n",
      "                     'chain_end': 0,\n",
      "                     'chain_start': 0,\n",
      "                     'dTm': 0,\n",
      "                     'dataset_source': 0,\n",
      "                     'ddG': 4553,\n",
      "                     'infos_found': 0,\n",
      "                     'length': 0,\n",
      "                     'mutated_aa': 0,\n",
      "                     'mutated_chain': 0,\n",
      "                     'mutation_position': 0,\n",
      "                     'pH': 0,\n",
      "                     'sequence': 0,\n",
      "                     'source_id': 0,\n",
      "                     'uniprot': 0,\n",
      "                     'wild_aa': 0},\n",
      " 'no_pH_repartition': {},\n",
      " 'total_len': 5146,\n",
      " 'unique_uniprot': 320}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_len': 5146,\n",
       " 'unique_uniprot': 320,\n",
       " 'ddG': 593,\n",
       " 'dTm': 5146,\n",
       " 'Tm': 3019,\n",
       " 'nan_repartition': {'uniprot': 0,\n",
       "  'wild_aa': 0,\n",
       "  'mutated_chain': 0,\n",
       "  'mutation_position': 0,\n",
       "  'mutated_aa': 0,\n",
       "  'pH': 0,\n",
       "  'sequence': 0,\n",
       "  'length': 0,\n",
       "  'chain_start': 0,\n",
       "  'chain_end': 0,\n",
       "  'AlphaFoldDB': 1049,\n",
       "  'Tm': 2127,\n",
       "  'ddG': 4553,\n",
       "  'dTm': 0,\n",
       "  'dataset_source': 0,\n",
       "  'source_id': 0,\n",
       "  'infos_found': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_csv(DATASET_OUTPUT_PATH_ONLY_INFOS)\n",
    "print(len(main_df))\n",
    "\n",
    "{\n",
    "    \"total_len\": len(main_df),\n",
    "    \"unique_uniprot\": len(main_df.uniprot.unique()),\n",
    "    \"ddG\": (len(main_df)-main_df.ddG.isna().sum()),\n",
    "    \"dTm\": (len(main_df)-main_df.dTm.isna().sum()),\n",
    "    \"Tm\": (len(main_df)-main_df.Tm.isna().sum()),\n",
    "    \"nan_repartition\": main_df.isna().sum().to_dict(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('novozymes-prediction-Gl9CRTFV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b13dc4add9efa918e6bb920c50afa2240557655d90455391ab57f21c65447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
